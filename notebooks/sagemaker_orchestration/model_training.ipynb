{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc07bab-8517-4cfc-beb1-807f1a2092f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T12:02:20.354259Z",
     "iopub.status.busy": "2025-08-13T12:02:20.354011Z",
     "iopub.status.idle": "2025-08-13T12:02:21.988438Z",
     "shell.execute_reply": "2025-08-13T12:02:21.987763Z",
     "shell.execute_reply.started": "2025-08-13T12:02:20.354239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.53.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.21.4.dev0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72803618-120f-41b2-b4b6-6b4ad0217d75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T12:02:21.990302Z",
     "iopub.status.busy": "2025-08-13T12:02:21.990075Z",
     "iopub.status.idle": "2025-08-13T12:02:23.633869Z",
     "shell.execute_reply": "2025-08-13T12:02:23.633015Z",
     "shell.execute_reply.started": "2025-08-13T12:02:21.990278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from datasets) (3.12.13)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.33.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.18.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /opt/conda/lib/python3.12/site-packages (from responses<0.19->datasets) (1.26.19)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "156f9590-48fa-4276-a087-775634155a11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T12:02:23.634978Z",
     "iopub.status.busy": "2025-08-13T12:02:23.634762Z",
     "iopub.status.idle": "2025-08-13T12:02:25.558340Z",
     "shell.execute_reply": "2025-08-13T12:02:25.557268Z",
     "shell.execute_reply.started": "2025-08-13T12:02:23.634956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.12/site-packages (2.250.0)\n",
      "Requirement already satisfied: attrs<26,>=24 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (25.3.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.35.36 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (1.37.1)\n",
      "Requirement already satisfied: cloudpickle>=2.2.1 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (3.1.1)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.12/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.12/site-packages (from sagemaker) (0.115.14)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.12/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: graphene<4,>=3 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (3.4.3)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (6.10.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.12/site-packages (from sagemaker) (4.23.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (1.26.4)\n",
      "Requirement already satisfied: omegaconf<3,>=2.2 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (2.3.0)\n",
      "Requirement already satisfied: packaging<25,>=23.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (24.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from sagemaker) (2.2.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.12/site-packages (from sagemaker) (0.3.4)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.12/site-packages (from sagemaker) (4.3.8)\n",
      "Requirement already satisfied: protobuf<6.32,>=3.12 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (5.28.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from sagemaker) (2.32.4)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.17 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (1.0.32)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.12/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from sagemaker) (4.67.1)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (1.26.19)\n",
      "Requirement already satisfied: uvicorn in /opt/conda/lib/python3.12/site-packages (from sagemaker) (0.35.0)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.1 in /opt/conda/lib/python3.12/site-packages (from boto3<2.0,>=1.35.36->sagemaker) (1.37.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3<2.0,>=1.35.36->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3<2.0,>=1.35.36->sagemaker) (0.11.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3<2.0,>=1.35.36->sagemaker) (2.9.0.post0)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /opt/conda/lib/python3.12/site-packages (from graphene<4,>=3->sagemaker) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /opt/conda/lib/python3.12/site-packages (from graphene<4,>=3->sagemaker) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7.1 in /opt/conda/lib/python3.12/site-packages (from graphene<4,>=3->sagemaker) (4.14.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.23.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/lib/python3.12/site-packages (from omegaconf<3,>=2.2->sagemaker) (4.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.1->boto3<2.0,>=1.35.36->sagemaker) (1.17.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.11.7)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (13.9.4)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema->sagemaker) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema->sagemaker) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema->sagemaker) (0.26.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->sagemaker) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->sagemaker) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->sagemaker) (2025.6.15)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /opt/conda/lib/python3.12/site-packages (from fastapi->sagemaker) (0.46.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /opt/conda/lib/python3.12/site-packages (from starlette<0.47.0,>=0.40.0->fastapi->sagemaker) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->sagemaker) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->sagemaker) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->sagemaker) (2025.2)\n",
      "Requirement already satisfied: ppft>=1.7.7 in /opt/conda/lib/python3.12/site-packages (from pathos->sagemaker) (1.7.7)\n",
      "Requirement already satisfied: dill>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pathos->sagemaker) (0.4.0)\n",
      "Requirement already satisfied: pox>=0.3.6 in /opt/conda/lib/python3.12/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.18 in /opt/conda/lib/python3.12/site-packages (from pathos->sagemaker) (0.70.18)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn->sagemaker) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.12/site-packages (from uvicorn->sagemaker) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce8a8b55-aa59-4af1-a9ce-1341f1d9c900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T12:02:25.559653Z",
     "iopub.status.busy": "2025-08-13T12:02:25.559417Z",
     "iopub.status.idle": "2025-08-13T12:02:27.176331Z",
     "shell.execute_reply": "2025-08-13T12:02:27.175523Z",
     "shell.execute_reply.started": "2025-08-13T12:02:25.559630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.12/site-packages (2024.12.0)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /opt/conda/lib/python3.12/site-packages (from s3fs) (2.21.1)\n",
      "Requirement already satisfied: fsspec==2024.12.0.* in /opt/conda/lib/python3.12/site-packages (from s3fs) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from s3fs) (3.12.13)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.37.2,>=1.37.0 in /opt/conda/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.37.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /opt/conda/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (6.6.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.6.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.20.1)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<1.37.2,>=1.37.0->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.1.2->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa2e217-7c43-4b8e-a084-ab1a878982e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T12:02:27.177915Z",
     "iopub.status.busy": "2025-08-13T12:02:27.177197Z",
     "iopub.status.idle": "2025-08-13T12:02:28.784003Z",
     "shell.execute_reply": "2025-08-13T12:02:28.783154Z",
     "shell.execute_reply.started": "2025-08-13T12:02:27.177884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: loguru in /opt/conda/lib/python3.12/site-packages (0.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fbff4c7-6220-4a64-9ee3-d4b89b270741",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T12:02:28.785571Z",
     "iopub.status.busy": "2025-08-13T12:02:28.785011Z",
     "iopub.status.idle": "2025-08-13T12:02:30.715367Z",
     "shell.execute_reply": "2025-08-13T12:02:30.714724Z",
     "shell.execute_reply.started": "2025-08-13T12:02:28.785535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13492ee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T12:02:30.716729Z",
     "iopub.status.busy": "2025-08-13T12:02:30.716434Z",
     "iopub.status.idle": "2025-08-13T12:02:30.720891Z",
     "shell.execute_reply": "2025-08-13T12:02:30.720090Z",
     "shell.execute_reply.started": "2025-08-13T12:02:30.716700Z"
    }
   },
   "outputs": [],
   "source": [
    "# IAM role for SageMaker\n",
    "# iam_role = \"arn:aws:iam::551529993308:role/service-role/AmazonSageMaker-ExecutionRole-20250711T075198\"\n",
    "iam_role = os.getenv(\"SAGEMAKER_IAM_ROLE\")\n",
    "# # S3 bucket for data and model artifacts\n",
    "# s3_bucket = \"self-corrective-llm-data\" \n",
    "s3_bucket = os.getenv(\"S3_BUCKET\")\n",
    "\n",
    "# try:\n",
    "# \trole = sagemaker.get_execution_role()\n",
    "#     # role = \"arn:aws:iam::551529993308:role/service-role/AmazonSageMaker-ExecutionRole-20250711T075198\"\n",
    "# except ValueError:\n",
    "# \tiam = boto3.client('iam')\n",
    "# \trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdb8d9ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T12:02:30.722307Z",
     "iopub.status.busy": "2025-08-13T12:02:30.722020Z",
     "iopub.status.idle": "2025-08-13T12:02:30.726059Z",
     "shell.execute_reply": "2025-08-13T12:02:30.725424Z",
     "shell.execute_reply.started": "2025-08-13T12:02:30.722279Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define S3 paths\n",
    "base_s3_uri = f\"s3://{s3_bucket}\"\n",
    "base_model_s3_uri = f\"{base_s3_uri}/self-corrective-llm-not-trained\"\n",
    "dataset_s3_uri = f\"{base_s3_uri}/dataset/training_data\"\n",
    "output_s3_uri = f\"{base_s3_uri}/trained_model/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f7fc57a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T12:02:30.727590Z",
     "iopub.status.busy": "2025-08-13T12:02:30.727131Z",
     "iopub.status.idle": "2025-08-13T12:02:30.965093Z",
     "shell.execute_reply": "2025-08-13T12:02:30.964378Z",
     "shell.execute_reply.started": "2025-08-13T12:02:30.727561Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "hyperparameters = {\n",
    "    # Core parameters\n",
    "    \"epochs\": 1,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"alpha\": 0.7,\n",
    "    \"pos_weight\": 10.0,\n",
    "\n",
    "    # Batching and memory\n",
    "    \"train_batch_size\": 1,\n",
    "    \"eval_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "\n",
    "    # LoRA parameters\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    \"optim\": \"paged_adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "\n",
    "    # Logging and saving\n",
    "    \"logging_steps\": 10,\n",
    "    \"eval_steps\": 50,\n",
    "    \"save_steps\": 50,\n",
    "}\n",
    "\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "environment = {\n",
    "    \"WANDB_API_KEY\": wandb_api_key,\n",
    "}\n",
    "\n",
    "# --- SageMaker Estimator ---\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",          # Your training script\n",
    "    source_dir=\"../../scripts\",         # Directory containing the script\n",
    "    instance_type=\"ml.g5.12xlarge\",   # Instance type for training\n",
    "    volume_size=100,\n",
    "    instance_count=1,\n",
    "    role=iam_role,\n",
    "    transformers_version=\"4.49.0\",     # Version of transformers\n",
    "    pytorch_version=\"2.5.1\",           # Version of PyTorch\n",
    "    py_version=\"py311\",              # Python version\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=output_s3_uri,\n",
    "    environment=environment,\n",
    "    # Dependencies from your project\n",
    "    dependencies=[\"../../src\"],\n",
    "    # Input channels for data and base model\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    # input_mode='File',\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}}\n",
    "    # distribution={\n",
    "    #     \"smdistributed\": {\n",
    "    #         \"modelparallel\": {\"enabled\": True, \"parameters\": {}} # Optional parameters\n",
    "    #     }\n",
    "    # }\n",
    "    # distribution={\n",
    "    #     \"parameter_server\": {\n",
    "    #         \"enabled\": True\n",
    "    #     }\n",
    "    # }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6531de7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T12:02:30.966396Z",
     "iopub.status.busy": "2025-08-13T12:02:30.966100Z",
     "iopub.status.idle": "2025-08-13T12:11:13.866716Z",
     "shell.execute_reply": "2025-08-13T12:11:13.866070Z",
     "shell.execute_reply.started": "2025-08-13T12:02:30.966369Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2025-08-13-12-02-30-992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 12:02:31 Starting - Starting the training job...\n",
      "2025-08-13 12:02:44 Pending - Training job waiting for capacity......\n",
      "2025-08-13 12:03:37 Pending - Preparing the instances for training...\n",
      "2025-08-13 12:04:11 Downloading - Downloading input data.........\n",
      "2025-08-13 12:05:41 Downloading - Downloading the training image............\n",
      "2025-08-13 12:07:53 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34mCUDA compat package should be installed for NVIDIA driver smaller than 550.163.01\u001b[0m\n",
      "\u001b[34mCurrent installed NVIDIA driver version is 570.172.08\u001b[0m\n",
      "\u001b[34mSkipping CUDA compat setup as newer NVIDIA driver is installed\u001b[0m\n",
      "\u001b[34m2025-08-13 12:08:42,792 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-08-13 12:08:42,829 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-08-13 12:08:42,838 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-08-13 12:08:42,840 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2025-08-13 12:08:42,840 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-08-13 12:08:44,308 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting datasets==4.0.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting wandb (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.21.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 1)) (3.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 1)) (21.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 1)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 1)) (2.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 1)) (2.32.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 1)) (4.66.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 1)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 1)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 1)) (2024.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 1)) (0.29.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 1)) (24.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 1)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 1)) (3.12.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 2)) (8.2.1)\u001b[0m\n",
      "\u001b[34mCollecting gitpython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 2)) (4.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/conda/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 2)) (6.31.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<3 in /opt/conda/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 2)) (2.11.7)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=2.0.0 (from wandb->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-2.34.1-py2.py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions<5,>=4.8 in /opt/conda/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 2)) (4.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3->wandb->-r requirements.txt (line 2)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3->wandb->-r requirements.txt (line 2)) (2.33.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3->wandb->-r requirements.txt (line 2)) (0.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets==4.0.0->-r requirements.txt (line 1)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets==4.0.0->-r requirements.txt (line 1)) (3.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets==4.0.0->-r requirements.txt (line 1)) (2.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets==4.0.0->-r requirements.txt (line 1)) (2025.7.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 1)) (2.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 1)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 1)) (25.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 1)) (1.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 1)) (6.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 1)) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 1)) (1.20.1)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==4.0.0->-r requirements.txt (line 1)) (2.9.0.post0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==4.0.0->-r requirements.txt (line 1)) (2025.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==4.0.0->-r requirements.txt (line 1)) (2025.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==4.0.0->-r requirements.txt (line 1)) (1.17.0)\u001b[0m\n",
      "\u001b[34mDownloading datasets-4.0.0-py3-none-any.whl (494 kB)\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.21.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.4/22.4 MB 196.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gitpython-3.1.45-py3-none-any.whl (208 kB)\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.12-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.2-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-2.34.1-py2.py3-none-any.whl (357 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: smmap, sentry-sdk, gitdb, gitpython, wandb, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 3.3.2\u001b[0m\n",
      "\u001b[34mUninstalling datasets-3.3.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-3.3.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed datasets-4.0.0 gitdb-4.0.12 gitpython-3.1.45 sentry-sdk-2.34.1 smmap-5.0.2 wandb-0.21.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 25.1.1 -> 25.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2025-08-13 12:08:47,810 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-08-13 12:08:47,810 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-08-13 12:08:47,872 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-08-13 12:08:47,921 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-08-13 12:08:47,930 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2025-08-13 12:08:47,969 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-08-13 12:08:47,979 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"dataset\": \"/opt/ml/input/data/dataset\",\n",
      "        \"model\": \"/opt/ml/input/data/model\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"alpha\": 0.7,\n",
      "        \"epochs\": 1,\n",
      "        \"eval_batch_size\": 2,\n",
      "        \"eval_steps\": 50,\n",
      "        \"gradient_accumulation_steps\": 8,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lora_alpha\": 32,\n",
      "        \"lora_dropout\": 0.05,\n",
      "        \"lora_r\": 16,\n",
      "        \"lr_scheduler_type\": \"cosine\",\n",
      "        \"optim\": \"paged_adamw_8bit\",\n",
      "        \"pos_weight\": 10.0,\n",
      "        \"save_steps\": 50,\n",
      "        \"train_batch_size\": 1,\n",
      "        \"weight_decay\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"dataset\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"model\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2025-08-13-12-02-30-992\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://self-corrective-llm-data/huggingface-pytorch-training-2025-08-13-12-02-30-992/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"alpha\":0.7,\"epochs\":1,\"eval_batch_size\":2,\"eval_steps\":50,\"gradient_accumulation_steps\":8,\"learning_rate\":0.0002,\"logging_steps\":10,\"lora_alpha\":32,\"lora_dropout\":0.05,\"lora_r\":16,\"lr_scheduler_type\":\"cosine\",\"optim\":\"paged_adamw_8bit\",\"pos_weight\":10.0,\"save_steps\":50,\"train_batch_size\":1,\"weight_decay\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.12xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"dataset\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"dataset\",\"model\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://self-corrective-llm-data/huggingface-pytorch-training-2025-08-13-12-02-30-992/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.12xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"dataset\":\"/opt/ml/input/data/dataset\",\"model\":\"/opt/ml/input/data/model\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"alpha\":0.7,\"epochs\":1,\"eval_batch_size\":2,\"eval_steps\":50,\"gradient_accumulation_steps\":8,\"learning_rate\":0.0002,\"logging_steps\":10,\"lora_alpha\":32,\"lora_dropout\":0.05,\"lora_r\":16,\"lr_scheduler_type\":\"cosine\",\"optim\":\"paged_adamw_8bit\",\"pos_weight\":10.0,\"save_steps\":50,\"train_batch_size\":1,\"weight_decay\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"dataset\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2025-08-13-12-02-30-992\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://self-corrective-llm-data/huggingface-pytorch-training-2025-08-13-12-02-30-992/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--alpha\",\"0.7\",\"--epochs\",\"1\",\"--eval_batch_size\",\"2\",\"--eval_steps\",\"50\",\"--gradient_accumulation_steps\",\"8\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"10\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"16\",\"--lr_scheduler_type\",\"cosine\",\"--optim\",\"paged_adamw_8bit\",\"--pos_weight\",\"10.0\",\"--save_steps\",\"50\",\"--train_batch_size\",\"1\",\"--weight_decay\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_DATASET=/opt/ml/input/data/dataset\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_HP_ALPHA=0.7\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_STEPS=50\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=16\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=cosine\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIM=paged_adamw_8bit\u001b[0m\n",
      "\u001b[34mSM_HP_POS_WEIGHT=10.0\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STEPS=50\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python311.zip:/opt/conda/lib/python3.11:/opt/conda/lib/python3.11/lib-dynload:/opt/conda/lib/python3.11/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 1 --nproc_per_node 4 train.py --alpha 0.7 --epochs 1 --eval_batch_size 2 --eval_steps 50 --gradient_accumulation_steps 8 --learning_rate 0.0002 --logging_steps 10 --lora_alpha 32 --lora_dropout 0.05 --lora_r 16 --lr_scheduler_type cosine --optim paged_adamw_8bit --pos_weight 10.0 --save_steps 50 --train_batch_size 1 --weight_decay 0.01\u001b[0m\n",
      "\u001b[34mW0813 12:08:49.265000 177 site-packages/torch/distributed/run.py:793] \u001b[0m\n",
      "\u001b[34mW0813 12:08:49.265000 177 site-packages/torch/distributed/run.py:793] *****************************************\u001b[0m\n",
      "\u001b[34mW0813 12:08:49.265000 177 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34mW0813 12:08:49.265000 177 site-packages/torch/distributed/run.py:793] *****************************************\u001b[0m\n",
      "\u001b[34m--- Loading tokenizer ---\u001b[0m\n",
      "\u001b[34m--- Loading BNB Config ---\u001b[0m\n",
      "\u001b[34m--- Loading Model with BNB Config ---\u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now default to True since model is quantized.\u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now default to True since model is quantized.\u001b[0m\n",
      "\u001b[34m--- Loading tokenizer ------ Loading tokenizer ------ Loading tokenizer ---\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--- Loading BNB Config ---\u001b[0m\n",
      "\u001b[34m--- Loading BNB Config ---\u001b[0m\n",
      "\u001b[34m--- Loading Model with BNB Config ---\u001b[0m\n",
      "\u001b[34m--- Loading Model with BNB Config ---\u001b[0m\n",
      "\u001b[34m--- Loading BNB Config ---\u001b[0m\n",
      "\u001b[34m--- Loading Model with BNB Config ---\u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now default to True since model is quantized.\u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now default to True since model is quantized.\u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now default to True since model is quantized.\u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now default to True since model is quantized.\u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now default to True since model is quantized.\u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now default to True since model is quantized.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.19s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.12s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.23s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.23s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.25s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:10<00:11,  5.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.25s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.29s/it]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at /opt/ml/input/data/model were not used when initializing SelfCorrectiveLlama: {'hallucination_detector.0.weight', 'hallucination_detector.0.bias'}\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at /opt/ml/input/data/model were not used when initializing SelfCorrectiveLlama: {'hallucination_detector.0.weight', 'hallucination_detector.0.bias'}\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of SelfCorrectiveLlama were not initialized from the model checkpoint at /opt/ml/input/data/model and are newly initialized: ['hallucination_detector.bias', 'hallucination_detector.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of SelfCorrectiveLlama were not initialized from the model checkpoint at /opt/ml/input/data/model and are newly initialized: ['hallucination_detector.bias', 'hallucination_detector.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m--- Prepare model for kbit training ---\u001b[0m\n",
      "\u001b[34m--- Configuring PEFT ---\u001b[0m\n",
      "\u001b[34m--- Applying PEFT ---\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.63s/it]#015Loading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.64s/it]\u001b[0m\n",
      "\u001b[34mtrainable params: 44,064,817 || all params: 8,074,354,738 || trainable%: 0.5457\u001b[0m\n",
      "\u001b[34m--- Loading dataset ---\u001b[0m\n",
      "\u001b[34mDatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'hallucination_labels'],\n",
      "        num_rows: 42049\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'hallucination_labels'],\n",
      "        num_rows: 4673\n",
      "    })\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34m--- Setting up Trainer ---\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/ml/code/src/trainer.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SelfCorrectionTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\u001b[0m\n",
      "\u001b[34mNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\u001b[0m\n",
      "\u001b[34m--- Starting training ---\u001b[0m\n",
      "\u001b[34mNCCL version 2.23.4+cuda12.4\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.16s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.65s/it]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at /opt/ml/input/data/model were not used when initializing SelfCorrectiveLlama: {'hallucination_detector.0.bias', 'hallucination_detector.0.weight'}\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at /opt/ml/input/data/model were not used when initializing SelfCorrectiveLlama: {'hallucination_detector.0.bias', 'hallucination_detector.0.weight'}\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of SelfCorrectiveLlama were not initialized from the model checkpoint at /opt/ml/input/data/model and are newly initialized: ['hallucination_detector.bias', 'hallucination_detector.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of SelfCorrectiveLlama were not initialized from the model checkpoint at /opt/ml/input/data/model and are newly initialized: ['hallucination_detector.bias', 'hallucination_detector.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.17s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.66s/it]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at /opt/ml/input/data/model were not used when initializing SelfCorrectiveLlama: {'hallucination_detector.0.bias', 'hallucination_detector.0.weight'}\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at /opt/ml/input/data/model were not used when initializing SelfCorrectiveLlama: {'hallucination_detector.0.bias', 'hallucination_detector.0.weight'}\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of SelfCorrectiveLlama were not initialized from the model checkpoint at /opt/ml/input/data/model and are newly initialized: ['hallucination_detector.bias', 'hallucination_detector.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of SelfCorrectiveLlama were not initialized from the model checkpoint at /opt/ml/input/data/model and are newly initialized: ['hallucination_detector.bias', 'hallucination_detector.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.18s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.66s/it]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at /opt/ml/input/data/model were not used when initializing SelfCorrectiveLlama: {'hallucination_detector.0.bias', 'hallucination_detector.0.weight'}\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at /opt/ml/input/data/model were not used when initializing SelfCorrectiveLlama: {'hallucination_detector.0.bias', 'hallucination_detector.0.weight'}\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing SelfCorrectiveLlama from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of SelfCorrectiveLlama were not initialized from the model checkpoint at /opt/ml/input/data/model and are newly initialized: ['hallucination_detector.bias', 'hallucination_detector.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of SelfCorrectiveLlama were not initialized from the model checkpoint at /opt/ml/input/data/model and are newly initialized: ['hallucination_detector.bias', 'hallucination_detector.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m--- Prepare model for kbit training ------ Prepare model for kbit training ---\u001b[0m\n",
      "\u001b[34m--- Prepare model for kbit training ---\u001b[0m\n",
      "\u001b[34m--- Configuring PEFT ---\u001b[0m\n",
      "\u001b[34m--- Applying PEFT ---\u001b[0m\n",
      "\u001b[34m--- Configuring PEFT ------ Configuring PEFT ---\u001b[0m\n",
      "\u001b[34m--- Applying PEFT ------ Applying PEFT ---\u001b[0m\n",
      "\u001b[34mtrainable params: 44,064,817 || all params: 8,074,354,738 || trainable%: 0.5457\u001b[0m\n",
      "\u001b[34m--- Loading dataset ---\u001b[0m\n",
      "\u001b[34mtrainable params: 44,064,817 || all params: 8,074,354,738 || trainable%: 0.5457\u001b[0m\n",
      "\u001b[34m--- Loading dataset ---\u001b[0m\n",
      "\u001b[34mtrainable params: 44,064,817 || all params: 8,074,354,738 || trainable%: 0.5457\u001b[0m\n",
      "\u001b[34m--- Loading dataset ---\u001b[0m\n",
      "\u001b[34mDatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'hallucination_labels'],\n",
      "        num_rows: 42049\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'hallucination_labels'],\n",
      "        num_rows: 4673\n",
      "    })\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34m--- Setting up Trainer ---\u001b[0m\n",
      "\u001b[34mDatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'hallucination_labels'],\n",
      "        num_rows: 42049\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'hallucination_labels'],\n",
      "        num_rows: 4673\n",
      "    })\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34m--- Setting up Trainer ---\u001b[0m\n",
      "\u001b[34mDatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'hallucination_labels'],\n",
      "        num_rows: 42049\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'hallucination_labels'],\n",
      "        num_rows: 4673\n",
      "    })\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34m--- Setting up Trainer ---\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/ml/code/src/trainer.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SelfCorrectionTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m/opt/ml/code/src/trainer.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SelfCorrectionTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m/opt/ml/code/src/trainer.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SelfCorrectionTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\u001b[0m\n",
      "\u001b[34mNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\u001b[0m\n",
      "\u001b[34m--- Starting training ---\u001b[0m\n",
      "\u001b[34mNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\u001b[0m\n",
      "\u001b[34mNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\u001b[0m\n",
      "\u001b[34m--- Starting training ---\u001b[0m\n",
      "\u001b[34mNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\u001b[0m\n",
      "\u001b[34mNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\u001b[0m\n",
      "\u001b[34m--- Starting training ---\u001b[0m\n",
      "\u001b[34mwandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\u001b[0m\n",
      "\u001b[34mYou're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: kyrylldekanenko (kyrylldekanenko-) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.21.1\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20250813_120917-huggingface-pytorch-training-2025-08-13-12-02-30-992-sgohz1-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run /opt/ml/model\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/kyrylldekanenko-/huggingface\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/kyrylldekanenko-/huggingface/runs/huggingface-pytorch-training-2025-08-13-12-02-30-992-sgohz1-algo-1\u001b[0m\n",
      "\u001b[34m0%|          | 0/1314 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mYou're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.4702128171920776, 'hallucination_loss': 2.328796863555908, 'epoch': 0}\u001b[0m\n",
      "\u001b[34m0%|          | 0/1314 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.6056891679763794, 'hallucination_loss': 1.2456458806991577, 'epoch': 0}\u001b[0m\n",
      "\u001b[34m0%|          | 0/1314 [00:01<?, ?it/s]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.2998296022415161, 'hallucination_loss': 1.6563758850097656, 'epoch': 0}\u001b[0m\n",
      "\u001b[34m0%|          | 0/1314 [00:03<?, ?it/s]\u001b[0m\n",
      "\u001b[34m{'token_loss': 3.3181777000427246, 'hallucination_loss': 1.525735855102539, 'epoch': 0}\u001b[0m\n",
      "\u001b[34m0%|          | 0/1314 [00:04<?, ?it/s]\u001b[0m\n",
      "\u001b[34m{'token_loss': 2.123187780380249, 'hallucination_loss': 1.3646631240844727, 'epoch': 0}\u001b[0m\n",
      "\u001b[34m0%|          | 0/1314 [00:05<?, ?it/s]\u001b[0m\n",
      "\u001b[34m{'token_loss': 9.549540519714355, 'hallucination_loss': 1.4553632736206055, 'epoch': 0}\u001b[0m\n",
      "\u001b[34m0%|          | 0/1314 [00:06<?, ?it/s]\u001b[0m\n",
      "\u001b[34m{'token_loss': 2.202533006668091, 'hallucination_loss': 1.1499826908111572, 'epoch': 0}\u001b[0m\n",
      "\u001b[34m0%|          | 0/1314 [00:07<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[rank3]:[W813 12:09:27.271527752 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[rank0]:[W813 12:09:27.576811855 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.9187573194503784, 'hallucination_loss': 0.8652318716049194, 'epoch': 0}\u001b[0m\n",
      "\u001b[34m0%|          | 0/1314 [00:08<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[rank1]:[W813 12:09:27.772162324 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[rank2]:[W813 12:09:28.572988016 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m0%|          | 1/1314 [00:10<3:53:08, 10.65s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.0402040481567383, 'hallucination_loss': 0.7461943030357361, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 1/1314 [00:11<3:53:08, 10.65s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.31341153383255005, 'hallucination_loss': 0.5089656114578247, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 1/1314 [00:12<3:53:08, 10.65s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.0394504070281982, 'hallucination_loss': 0.7523921728134155, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 1/1314 [00:14<3:53:08, 10.65s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.1044633388519287, 'hallucination_loss': 0.7621989846229553, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 1/1314 [00:15<3:53:08, 10.65s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 8.932401657104492, 'hallucination_loss': 2.9707388877868652, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 1/1314 [00:16<3:53:08, 10.65s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.7203916311264038, 'hallucination_loss': 1.1269731521606445, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 1/1314 [00:17<3:53:08, 10.65s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.9359166026115417, 'hallucination_loss': 1.4026790857315063, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 1/1314 [00:18<3:53:08, 10.65s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 2.458732843399048, 'hallucination_loss': 3.2456207275390625, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 1/1314 [00:19<3:53:08, 10.65s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/1314 [00:21<3:49:44, 10.51s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.9469428062438965, 'hallucination_loss': 0.87660151720047, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 2/1314 [00:21<3:49:44, 10.51s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.4128488302230835, 'hallucination_loss': 0.3052390217781067, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 2/1314 [00:22<3:49:44, 10.51s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.4503240585327148, 'hallucination_loss': 2.017021894454956, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 2/1314 [00:24<3:49:44, 10.51s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 2.1872971057891846, 'hallucination_loss': 3.616292953491211, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 2/1314 [00:25<3:49:44, 10.51s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.2088799476623535, 'hallucination_loss': 1.3813276290893555, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 2/1314 [00:26<3:49:44, 10.51s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.1548008918762207, 'hallucination_loss': 0.8840982913970947, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 2/1314 [00:27<3:49:44, 10.51s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.940200686454773, 'hallucination_loss': 0.33004701137542725, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 2/1314 [00:28<3:49:44, 10.51s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.8746519684791565, 'hallucination_loss': 1.5801646709442139, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 2/1314 [00:29<3:49:44, 10.51s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/1314 [00:30<3:43:38, 10.23s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.627609372138977, 'hallucination_loss': 2.0837814807891846, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 3/1314 [00:31<3:43:38, 10.23s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.5916634798049927, 'hallucination_loss': 0.036972589790821075, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 3/1314 [00:32<3:43:38, 10.23s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.46931833028793335, 'hallucination_loss': 0.042066432535648346, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 3/1314 [00:33<3:43:38, 10.23s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.1877870559692383, 'hallucination_loss': 0.13347655534744263, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 3/1314 [00:34<3:43:38, 10.23s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.4057427644729614, 'hallucination_loss': 0.785099446773529, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 3/1314 [00:36<3:43:38, 10.23s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.6018159985542297, 'hallucination_loss': 1.2912259101867676, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 3/1314 [00:37<3:43:38, 10.23s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.41661426424980164, 'hallucination_loss': 0.04570196568965912, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 3/1314 [00:38<3:43:38, 10.23s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.2783676385879517, 'hallucination_loss': 1.5329092741012573, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 3/1314 [00:39<3:43:38, 10.23s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/1314 [00:41<3:47:37, 10.43s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.468220591545105, 'hallucination_loss': 0.6778591275215149, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 4/1314 [00:42<3:47:37, 10.43s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.9083272218704224, 'hallucination_loss': 1.3005619049072266, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 4/1314 [00:43<3:47:37, 10.43s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.4819188416004181, 'hallucination_loss': 0.027479106560349464, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 4/1314 [00:44<3:47:37, 10.43s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 3.4818947315216064, 'hallucination_loss': 2.5495269298553467, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 4/1314 [00:45<3:47:37, 10.43s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.7962782382965088, 'hallucination_loss': 0.9168170690536499, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 4/1314 [00:47<3:47:37, 10.43s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.36166420578956604, 'hallucination_loss': 0.11100691556930542, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 4/1314 [00:48<3:47:37, 10.43s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.7250682711601257, 'hallucination_loss': 0.3699146807193756, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 4/1314 [00:49<3:47:37, 10.43s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.5181631445884705, 'hallucination_loss': 0.2346246838569641, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 4/1314 [00:50<3:47:37, 10.43s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 5/1314 [00:52<3:46:55, 10.40s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 3.1757750511169434, 'hallucination_loss': 5.337101459503174, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 5/1314 [00:52<3:46:55, 10.40s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.21155871450901031, 'hallucination_loss': 0.00441233953461051, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 5/1314 [00:53<3:46:55, 10.40s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.8030641078948975, 'hallucination_loss': 0.10786980390548706, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 5/1314 [00:54<3:46:55, 10.40s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.4839959442615509, 'hallucination_loss': 0.012824507430195808, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 5/1314 [00:55<3:46:55, 10.40s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.8276539444923401, 'hallucination_loss': 0.376345694065094, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 5/1314 [00:57<3:46:55, 10.40s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 2.1142966747283936, 'hallucination_loss': 2.0863022804260254, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 5/1314 [00:58<3:46:55, 10.40s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.24240675568580627, 'hallucination_loss': 0.10288781672716141, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 5/1314 [00:59<3:46:55, 10.40s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.5810005068778992, 'hallucination_loss': 0.0736892819404602, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 5/1314 [01:00<3:46:55, 10.40s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 6/1314 [01:01<3:42:25, 10.20s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 2.868168592453003, 'hallucination_loss': 5.974813938140869, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 6/1314 [01:02<3:42:25, 10.20s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.4929344952106476, 'hallucination_loss': 0.04969760403037071, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 6/1314 [01:03<3:42:25, 10.20s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.38914960622787476, 'hallucination_loss': 0.15571989119052887, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 6/1314 [01:04<3:42:25, 10.20s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.3501693308353424, 'hallucination_loss': 0.05940568447113037, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 6/1314 [01:05<3:42:25, 10.20s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.8710212707519531, 'hallucination_loss': 0.4122515618801117, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 6/1314 [01:07<3:42:25, 10.20s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.6591120958328247, 'hallucination_loss': 0.051872313022613525, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 6/1314 [01:08<3:42:25, 10.20s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.4799966812133789, 'hallucination_loss': 0.011682961136102676, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 6/1314 [01:09<3:42:25, 10.20s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.1822686195373535, 'hallucination_loss': 0.08174515515565872, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 6/1314 [01:10<3:42:25, 10.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 7/1314 [01:11<3:40:12, 10.11s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.31675249338150024, 'hallucination_loss': 1.8409276008605957, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 7/1314 [01:12<3:40:12, 10.11s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.5029659271240234, 'hallucination_loss': 1.974515676498413, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 7/1314 [01:13<3:40:12, 10.11s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.9094981551170349, 'hallucination_loss': 0.7862990498542786, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 7/1314 [01:14<3:40:12, 10.11s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.4357612133026123, 'hallucination_loss': 0.6723056435585022, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 7/1314 [01:15<3:40:12, 10.11s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.5106589794158936, 'hallucination_loss': 0.09479274600744247, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 7/1314 [01:16<3:40:12, 10.11s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.30927523970603943, 'hallucination_loss': 0.1108001321554184, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 7/1314 [01:18<3:40:12, 10.11s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.45796093344688416, 'hallucination_loss': 0.7960283756256104, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 7/1314 [01:19<3:40:12, 10.11s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.2847791910171509, 'hallucination_loss': 0.41222912073135376, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 7/1314 [01:20<3:40:12, 10.11s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 8/1314 [01:22<3:43:25, 10.26s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.9571126103401184, 'hallucination_loss': 0.12616635859012604, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 8/1314 [01:22<3:43:25, 10.26s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.5066074728965759, 'hallucination_loss': 0.36912935972213745, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 8/1314 [01:23<3:43:25, 10.26s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.5892727375030518, 'hallucination_loss': 0.42804330587387085, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 8/1314 [01:25<3:43:25, 10.26s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.0091369152069092, 'hallucination_loss': 1.3432188034057617, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 8/1314 [01:26<3:43:25, 10.26s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.4260445833206177, 'hallucination_loss': 0.9166125059127808, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 8/1314 [01:27<3:43:25, 10.26s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.3899230659008026, 'hallucination_loss': 0.709592878818512, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 8/1314 [01:28<3:43:25, 10.26s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.9605875611305237, 'hallucination_loss': 1.1400434970855713, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 8/1314 [01:29<3:43:25, 10.26s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.44984012842178345, 'hallucination_loss': 0.1724574714899063, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 8/1314 [01:30<3:43:25, 10.26s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 9/1314 [01:32<3:43:29, 10.28s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.1209754943847656, 'hallucination_loss': 1.3092049360275269, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 9/1314 [01:33<3:43:29, 10.28s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.5162645578384399, 'hallucination_loss': 0.0645948126912117, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 9/1314 [01:34<3:43:29, 10.28s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.2812832295894623, 'hallucination_loss': 0.12355032563209534, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 9/1314 [01:35<3:43:29, 10.28s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.6167005300521851, 'hallucination_loss': 0.16853077709674835, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 9/1314 [01:36<3:43:29, 10.28s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.2418049573898315, 'hallucination_loss': 0.13970790803432465, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 9/1314 [01:37<3:43:29, 10.28s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.2774812579154968, 'hallucination_loss': 0.061192065477371216, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 9/1314 [01:38<3:43:29, 10.28s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.46639758348464966, 'hallucination_loss': 0.058822207152843475, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 9/1314 [01:39<3:43:29, 10.28s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 1.1091034412384033, 'hallucination_loss': 1.0222299098968506, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 9/1314 [01:40<3:43:29, 10.28s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 10/1314 [01:42<3:42:50, 10.25s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 8.6888, 'grad_norm': 19.9941349029541, 'learning_rate': 0.00019997142025618701, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 10/1314 [01:42<3:42:50, 10.25s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 2.4484658241271973, 'hallucination_loss': 4.353118896484375, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 10/1314 [01:43<3:42:50, 10.25s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.7313116788864136, 'hallucination_loss': 0.05985501781105995, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 10/1314 [01:44<3:42:50, 10.25s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.9176430702209473, 'hallucination_loss': 1.32624089717865, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 10/1314 [01:45<3:42:50, 10.25s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.7356621026992798, 'hallucination_loss': 0.0616035982966423, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 10/1314 [01:46<3:42:50, 10.25s/it]\u001b[0m\n",
      "\u001b[34m{'token_loss': 0.6232561469078064, 'hallucination_loss': 0.016652336344122887, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 10/1314 [01:47<3:42:50, 10.25s/it]\u001b[0m\n",
      "SageMaker training job started.\n"
     ]
    }
   ],
   "source": [
    "# --- Start Training ---\n",
    "huggingface_estimator.fit({\n",
    "    \"dataset\": dataset_s3_uri,\n",
    "    \"model\": base_model_s3_uri\n",
    "})\n",
    "\n",
    "print(\"SageMaker training job started.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3757f-e646-4ecf-b536-b156f7b223f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
