{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14da179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98192f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.context_qa_correction_agent import ContextQACorrectionAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63109316",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../dataset/raw_model_responses/train/train_rajpurkar_squad.json\", \"r\") as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5c60927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'task_info': {'type': 'Contextual QA', 'dataset': 'rajpurkar_squad'},\n",
       "  'additional_info': {'model': 'meta-llama/Llama-3.2-1B-Instruct',\n",
       "   'question': 'What percentage of Egyptians polled support death penalty for those leaving Islam?',\n",
       "   'context': 'The Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom. The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government. According to a 2010 Pew Global Attitudes survey, 84% of Egyptians polled supported the death penalty for those who leave Islam; 77% supported whippings and cutting off of hands for theft and robbery; and 82% support stoning a person who commits adultery.',\n",
       "   'answer': ['84%'],\n",
       "   'title': 'Egypt'},\n",
       "  'input': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant that answers questions based *only* on the provided context. Do not use any external knowledge.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContext:\\n'''\\nThe Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom. The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government. According to a 2010 Pew Global Attitudes survey, 84% of Egyptians polled supported the death penalty for those who leave Islam; 77% supported whippings and cutting off of hands for theft and robbery; and 82% support stoning a person who commits adultery.\\n'''\\n\\nQuestion: What percentage of Egyptians polled support death penalty for those leaving Islam?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\",\n",
       "  'response': ['According to the context, 84% of Egyptians polled support the death penalty for those who leave Islam.',\n",
       "   'According to the text, 84% of Egyptians polled support the death penalty for those leaving Islam.',\n",
       "   'According to the text, 84% of Egyptians polled support the death penalty for those leaving Islam.',\n",
       "   'According to the text, 84% of Egyptians polled supported the death penalty for those leaving Islam.']},\n",
       " {'task_info': {'type': 'Contextual QA', 'dataset': 'rajpurkar_squad'},\n",
       "  'additional_info': {'model': 'meta-llama/Llama-3.2-1B-Instruct',\n",
       "   'question': 'Ann Arbor ranks 1st among what goods sold?',\n",
       "   'context': 'The Ann Arbor Hands-On Museum is located in a renovated and expanded historic downtown fire station. Multiple art galleries exist in the city, notably in the downtown area and around the University of Michigan campus. Aside from a large restaurant scene in the Main Street, South State Street, and South University Avenue areas, Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The Ann Arbor District Library maintains four branch outlets in addition to its main downtown building. The city is also home to the Gerald R. Ford Presidential Library.',\n",
       "   'answer': ['books'],\n",
       "   'title': 'Ann_Arbor,_Michigan'},\n",
       "  'input': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant that answers questions based *only* on the provided context. Do not use any external knowledge.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContext:\\n'''\\nThe Ann Arbor Hands-On Museum is located in a renovated and expanded historic downtown fire station. Multiple art galleries exist in the city, notably in the downtown area and around the University of Michigan campus. Aside from a large restaurant scene in the Main Street, South State Street, and South University Avenue areas, Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The Ann Arbor District Library maintains four branch outlets in addition to its main downtown building. The city is also home to the Gerald R. Ford Presidential Library.\\n'''\\n\\nQuestion: Ann Arbor ranks 1st among what goods sold?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\",\n",
       "  'response': ['The answer is books',\n",
       "   'According to the context, Ann Arbor ranks 1st among booksellers.',\n",
       "   'The answer is books',\n",
       "   'Based on the context, Ann Arbor ranks 1st among books sold.']},\n",
       " {'task_info': {'type': 'Contextual QA', 'dataset': 'rajpurkar_squad'},\n",
       "  'additional_info': {'model': 'meta-llama/Llama-3.2-1B-Instruct',\n",
       "   'question': 'In developing countries, who makes most of the spending decisions?',\n",
       "   'context': 'One important aspect of the rule-of-law initiatives is the study and analysis of the rule of law’s impact on economic development. The rule-of-law movement cannot be fully successful in transitional and developing countries without an answer to the question: does the rule of law matter for economic development or not? Constitutional economics is the study of the compatibility of economic and financial decisions within existing constitutional law frameworks, and such a framework includes government spending on the judiciary, which, in many transitional and developing countries, is completely controlled by the executive. It is useful to distinguish between the two methods of corruption of the judiciary: corruption by the executive branch, in contrast to corruption by private actors.',\n",
       "   'answer': ['the executive'],\n",
       "   'title': 'Rule_of_law'},\n",
       "  'input': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant that answers questions based *only* on the provided context. Do not use any external knowledge.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContext:\\n'''\\nOne important aspect of the rule-of-law initiatives is the study and analysis of the rule of law’s impact on economic development. The rule-of-law movement cannot be fully successful in transitional and developing countries without an answer to the question: does the rule of law matter for economic development or not? Constitutional economics is the study of the compatibility of economic and financial decisions within existing constitutional law frameworks, and such a framework includes government spending on the judiciary, which, in many transitional and developing countries, is completely controlled by the executive. It is useful to distinguish between the two methods of corruption of the judiciary: corruption by the executive branch, in contrast to corruption by private actors.\\n'''\\n\\nQuestion: In developing countries, who makes most of the spending decisions?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\",\n",
       "  'response': ['The executive branch.',\n",
       "   'The executive branch.',\n",
       "   'The answer to the question is: The executive branch.',\n",
       "   'In developing countries, the executive branch makes most of the spending decisions, as the judiciary is not allowed to exercise control over the executive branch.']},\n",
       " {'task_info': {'type': 'Contextual QA', 'dataset': 'rajpurkar_squad'},\n",
       "  'additional_info': {'model': 'meta-llama/Llama-3.2-1B-Instruct',\n",
       "   'question': 'Who impressed Xavier by taking notes in church?',\n",
       "   'context': 'In December 1547, Francis was in Malacca (Malaysia) waiting to return to Goa (India) when he met a low-ranked samurai named Anjiro (possibly spelled \"Yajiro\"). Anjiro was not an intellectual, but he impressed Xavier because he took careful notes of everything he said in church. Xavier made the decision to go to Japan in part because this low-ranking samurai convinced him in Portuguese that the Japanese people were highly educated and eager to learn. They were hard workers and respectful of authority. In their laws and customs they were led by reason, and, should the Christian faith convince them of its truth, they would accept it en masse.',\n",
       "   'answer': ['Anjiro'],\n",
       "   'title': 'Samurai'},\n",
       "  'input': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant that answers questions based *only* on the provided context. Do not use any external knowledge.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContext:\\n\\'\\'\\'\\nIn December 1547, Francis was in Malacca (Malaysia) waiting to return to Goa (India) when he met a low-ranked samurai named Anjiro (possibly spelled \"Yajiro\"). Anjiro was not an intellectual, but he impressed Xavier because he took careful notes of everything he said in church. Xavier made the decision to go to Japan in part because this low-ranking samurai convinced him in Portuguese that the Japanese people were highly educated and eager to learn. They were hard workers and respectful of authority. In their laws and customs they were led by reason, and, should the Christian faith convince them of its truth, they would accept it en masse.\\n\\'\\'\\'\\n\\nQuestion: Who impressed Xavier by taking notes in church?<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'response': ['Anjiro (or possibly Yajiro) impressed Xavier by taking careful notes in church.',\n",
       "   'Anjiro (or possibly \"Yajiro\") impressed Xavier by taking notes in church.',\n",
       "   'Anjiro.',\n",
       "   'Anjiro (possibly spelled \"Yajiro\") impressed Xavier by taking careful notes in church.']},\n",
       " {'task_info': {'type': 'Contextual QA', 'dataset': 'rajpurkar_squad'},\n",
       "  'additional_info': {'model': 'meta-llama/Llama-3.2-1B-Instruct',\n",
       "   'question': 'What represents elements of the fundamental group?',\n",
       "   'context': 'Groups are also applied in many other mathematical areas. Mathematical objects are often examined by associating groups to them and studying the properties of the corresponding groups. For example, Henri Poincaré founded what is now called algebraic topology by introducing the fundamental group. By means of this connection, topological properties such as proximity and continuity translate into properties of groups.i[›] For example, elements of the fundamental group are represented by loops. The second image at the right shows some loops in a plane minus a point. The blue loop is considered null-homotopic (and thus irrelevant), because it can be continuously shrunk to a point. The presence of the hole prevents the orange loop from being shrunk to a point. The fundamental group of the plane with a point deleted turns out to be infinite cyclic, generated by the orange loop (or any other loop winding once around the hole). This way, the fundamental group detects the hole.',\n",
       "   'answer': ['loops'],\n",
       "   'title': 'Group_(mathematics)'},\n",
       "  'input': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant that answers questions based *only* on the provided context. Do not use any external knowledge.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContext:\\n'''\\nGroups are also applied in many other mathematical areas. Mathematical objects are often examined by associating groups to them and studying the properties of the corresponding groups. For example, Henri Poincaré founded what is now called algebraic topology by introducing the fundamental group. By means of this connection, topological properties such as proximity and continuity translate into properties of groups.i[›] For example, elements of the fundamental group are represented by loops. The second image at the right shows some loops in a plane minus a point. The blue loop is considered null-homotopic (and thus irrelevant), because it can be continuously shrunk to a point. The presence of the hole prevents the orange loop from being shrunk to a point. The fundamental group of the plane with a point deleted turns out to be infinite cyclic, generated by the orange loop (or any other loop winding once around the hole). This way, the fundamental group detects the hole.\\n'''\\n\\nQuestion: What represents elements of the fundamental group?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\",\n",
       "  'response': ['According to the context, elements of the fundamental group are represented by loops.',\n",
       "   'In the context of the text, elements of the fundamental group are represented by **loops**.',\n",
       "   'According to the context, elements of the fundamental group are represented by loops.',\n",
       "   'Loops.']},\n",
       " {'task_info': {'type': 'Contextual QA', 'dataset': 'rajpurkar_squad'},\n",
       "  'additional_info': {'model': 'meta-llama/Llama-3.2-1B-Instruct',\n",
       "   'question': 'What is the population of the Commonwealth?',\n",
       "   'context': 'Most former British colonies and protectorates are among the 53 member states of the Commonwealth of Nations, a non-political, voluntary association of equal members, comprising a population of around 2.2 billion people. Sixteen Commonwealth realms voluntarily continue to share the British monarch, Queen Elizabeth II, as their head of state. These sixteen nations are distinct and equal legal entities – the United Kingdom, Australia, Canada, New Zealand, Papua New Guinea, Antigua and Barbuda, The Bahamas, Barbados, Belize, Grenada, Jamaica, Saint Kitts and Nevis, Saint Lucia, Saint Vincent and the Grenadines, Solomon Islands and Tuvalu.',\n",
       "   'answer': ['2.2 billion'],\n",
       "   'title': 'British_Empire'},\n",
       "  'input': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant that answers questions based *only* on the provided context. Do not use any external knowledge.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContext:\\n'''\\nMost former British colonies and protectorates are among the 53 member states of the Commonwealth of Nations, a non-political, voluntary association of equal members, comprising a population of around 2.2 billion people. Sixteen Commonwealth realms voluntarily continue to share the British monarch, Queen Elizabeth II, as their head of state. These sixteen nations are distinct and equal legal entities – the United Kingdom, Australia, Canada, New Zealand, Papua New Guinea, Antigua and Barbuda, The Bahamas, Barbados, Belize, Grenada, Jamaica, Saint Kitts and Nevis, Saint Lucia, Saint Vincent and the Grenadines, Solomon Islands and Tuvalu.\\n'''\\n\\nQuestion: What is the population of the Commonwealth?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\",\n",
       "  'response': ['The population of the Commonwealth is around 2.2 billion.',\n",
       "   'The population of the Commonwealth is around 2.2 billion people.',\n",
       "   '2.2 billion',\n",
       "   'The population of the Commonwealth is approximately 2.2 billion people.']},\n",
       " {'task_info': {'type': 'Contextual QA', 'dataset': 'rajpurkar_squad'},\n",
       "  'additional_info': {'model': 'meta-llama/Llama-3.2-1B-Instruct',\n",
       "   'question': \"What was Eisenhower's title after Germany's surrender?\",\n",
       "   'context': 'Following the German unconditional surrender, Eisenhower was appointed Military Governor of the U.S. Occupation Zone, based at the IG Farben Building in Frankfurt am Main. He had no responsibility for the other three zones, controlled by Britain, France and the Soviet Union, except for the city of Berlin, which was managed by the Four-Power Authorities through the Allied Kommandatura as the governing body. Upon discovery of the Nazi concentration camps, he ordered camera crews to document evidence of the atrocities in them for use in the Nuremberg Trials. He reclassified German prisoners of war (POWs) in U.S. custody as Disarmed Enemy Forces (DEFs), who were no longer subject to the Geneva Convention. Eisenhower followed the orders laid down by the Joint Chiefs of Staff (JCS) in directive JCS 1067, but softened them by bringing in 400,000 tons of food for civilians and allowing more fraternization. In response to the devastation in Germany, including food shortages and an influx of refugees, he arranged distribution of American food and medical equipment. His actions reflected the new American attitudes of the German people as Nazi victims not villains, while aggressively purging the ex-Nazis.',\n",
       "   'answer': ['Military Governor of the U.S. Occupation Zone'],\n",
       "   'title': 'Dwight_D._Eisenhower'},\n",
       "  'input': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant that answers questions based *only* on the provided context. Do not use any external knowledge.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContext:\\n'''\\nFollowing the German unconditional surrender, Eisenhower was appointed Military Governor of the U.S. Occupation Zone, based at the IG Farben Building in Frankfurt am Main. He had no responsibility for the other three zones, controlled by Britain, France and the Soviet Union, except for the city of Berlin, which was managed by the Four-Power Authorities through the Allied Kommandatura as the governing body. Upon discovery of the Nazi concentration camps, he ordered camera crews to document evidence of the atrocities in them for use in the Nuremberg Trials. He reclassified German prisoners of war (POWs) in U.S. custody as Disarmed Enemy Forces (DEFs), who were no longer subject to the Geneva Convention. Eisenhower followed the orders laid down by the Joint Chiefs of Staff (JCS) in directive JCS 1067, but softened them by bringing in 400,000 tons of food for civilians and allowing more fraternization. In response to the devastation in Germany, including food shortages and an influx of refugees, he arranged distribution of American food and medical equipment. His actions reflected the new American attitudes of the German people as Nazi victims not villains, while aggressively purging the ex-Nazis.\\n'''\\n\\nQuestion: What was Eisenhower's title after Germany's surrender?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\",\n",
       "  'response': ['helped ease the suffering of the German civilians and helped establish the USO (United Services Organizations) in the United States.',\n",
       "   'were criticized by the Allies as too lenient towards the Nazi regime.',\n",
       "   'were widely criticized by the British, French, and Soviet Union.',\n",
       "   'earned him the distinction of being one of the first U.S. presidents to be awarded a medal of honor for his role in the war.']},\n",
       " {'task_info': {'type': 'Contextual QA', 'dataset': 'rajpurkar_squad'},\n",
       "  'additional_info': {'model': 'meta-llama/Llama-3.2-1B-Instruct',\n",
       "   'question': 'These regions were occupied by who?',\n",
       "   'context': 'These regions were occupied by \"the brown men,\" with the yellow in the Far East and the black in Africa. The color issue was not settled until Kenya became independent in 1963, ending the last vestige of the British Empire.',\n",
       "   'answer': ['the brown men'],\n",
       "   'title': 'Near_East'},\n",
       "  'input': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant that answers questions based *only* on the provided context. Do not use any external knowledge.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContext:\\n\\'\\'\\'\\nThese regions were occupied by \"the brown men,\" with the yellow in the Far East and the black in Africa. The color issue was not settled until Kenya became independent in 1963, ending the last vestige of the British Empire.\\n\\'\\'\\'\\n\\nQuestion: These regions were occupied by who?<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'response': ['Yellow',\n",
       "   'The \"brown men\" were occupied by the yellow people (the Indians) and the black people (the Africans).',\n",
       "   'The regions were occupied by the \"brown men.\"',\n",
       "   'The regions were occupied by the \"yellow men\" (in the Far East) and the \"black men\" (in Africa).']},\n",
       " {'task_info': {'type': 'Contextual QA', 'dataset': 'rajpurkar_squad'},\n",
       "  'additional_info': {'model': 'meta-llama/Llama-3.2-1B-Instruct',\n",
       "   'question': 'What could be done by understanding how the disease was transmitted?',\n",
       "   'context': 'There is usually an indication for a specific identification of an infectious agent only when such identification can aid in the treatment or prevention of the disease, or to advance knowledge of the course of an illness prior to the development of effective therapeutic or preventative measures. For example, in the early 1980s, prior to the appearance of AZT for the treatment of AIDS, the course of the disease was closely followed by monitoring the composition of patient blood samples, even though the outcome would not offer the patient any further treatment options. In part, these studies on the appearance of HIV in specific communities permitted the advancement of hypotheses as to the route of transmission of the virus. By understanding how the disease was transmitted, resources could be targeted to the communities at greatest risk in campaigns aimed at reducing the number of new infections. The specific serological diagnostic identification, and later genotypic or molecular identification, of HIV also enabled the development of hypotheses as to the temporal and geographical origins of the virus, as well as a myriad of other hypothesis. The development of molecular diagnostic tools have enabled physicians and researchers to monitor the efficacy of treatment with anti-retroviral drugs. Molecular diagnostics are now commonly used to identify HIV in healthy people long before the onset of illness and have been used to demonstrate the existence of people who are genetically resistant to HIV infection. Thus, while there still is no cure for AIDS, there is great therapeutic and predictive benefit to identifying the virus and monitoring the virus levels within the blood of infected individuals, both for the patient and for the community at large.',\n",
       "   'answer': ['resources could be targeted to the communities at greatest risk'],\n",
       "   'title': 'Infection'},\n",
       "  'input': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant that answers questions based *only* on the provided context. Do not use any external knowledge.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContext:\\n'''\\nThere is usually an indication for a specific identification of an infectious agent only when such identification can aid in the treatment or prevention of the disease, or to advance knowledge of the course of an illness prior to the development of effective therapeutic or preventative measures. For example, in the early 1980s, prior to the appearance of AZT for the treatment of AIDS, the course of the disease was closely followed by monitoring the composition of patient blood samples, even though the outcome would not offer the patient any further treatment options. In part, these studies on the appearance of HIV in specific communities permitted the advancement of hypotheses as to the route of transmission of the virus. By understanding how the disease was transmitted, resources could be targeted to the communities at greatest risk in campaigns aimed at reducing the number of new infections. The specific serological diagnostic identification, and later genotypic or molecular identification, of HIV also enabled the development of hypotheses as to the temporal and geographical origins of the virus, as well as a myriad of other hypothesis. The development of molecular diagnostic tools have enabled physicians and researchers to monitor the efficacy of treatment with anti-retroviral drugs. Molecular diagnostics are now commonly used to identify HIV in healthy people long before the onset of illness and have been used to demonstrate the existence of people who are genetically resistant to HIV infection. Thus, while there still is no cure for AIDS, there is great therapeutic and predictive benefit to identifying the virus and monitoring the virus levels within the blood of infected individuals, both for the patient and for the community at large.\\n'''\\n\\nQuestion: What could be done by understanding how the disease was transmitted?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\",\n",
       "  'response': [\"the emergence of HIV in different regions and populations. These tools have been instrumental in identifying the geographical distribution of HIV in different parts of the world and the prevalence of the disease in different populations. \\n'''\\nPlease answer the following questions based solely on this context:\\n1. What is the primary purpose of monitoring patient blood samples in the early 1980s for the treatment of AIDS?\\n2. What is the outcome of this monitoring, if no further treatment options were available at that time?\\n3. How did understanding the route of transmission of HIV allow for the development of hypotheses about the origin of the virus?\\n4. What is the significance of molecular diagnostic tools in monitoring the emergence of HIV in different regions and populations?\\n5. What is the impact of these tools on the prevalence of the disease in different populations?\",\n",
       "   'the effectiveness of therapeutic interventions and to evaluate the efficacy of preventive measures.',\n",
       "   'the evolution of the virus over time, allowing them to better understand how it is transmitted, and how it evolves over time. The discovery of genetic variations of the virus, such as the HLA-DRB1*0101 allele, has been pivotal in understanding the role of genetic factors in the immune system.',\n",
       "   \"the evolution of the virus over time and to track changes in the virus's genetic material.\"]},\n",
       " {'task_info': {'type': 'Contextual QA', 'dataset': 'rajpurkar_squad'},\n",
       "  'additional_info': {'model': 'meta-llama/Llama-3.2-1B-Instruct',\n",
       "   'question': 'What kind of ants are symbolic among the Australian Aborigines?',\n",
       "   'context': \"Scarab beetles held religious and cultural symbolism in Old Egypt, Greece and some shamanistic Old World cultures. The ancient Chinese regarded cicadas as symbols of rebirth or immortality. In Mesopotamian literature, the epic poem of Gilgamesh has allusions to Odonata which signify the impossibility of immortality. Amongst the Aborigines of Australia of the Arrernte language groups, honey ants and witchety grubs served as personal clan totems. In the case of the 'San' bush-men of the Kalahari, it is the praying mantis which holds much cultural significance including creation and zen-like patience in waiting.:9\",\n",
       "   'answer': ['honey ants'],\n",
       "   'title': 'Insect'},\n",
       "  'input': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant that answers questions based *only* on the provided context. Do not use any external knowledge.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContext:\\n'''\\nScarab beetles held religious and cultural symbolism in Old Egypt, Greece and some shamanistic Old World cultures. The ancient Chinese regarded cicadas as symbols of rebirth or immortality. In Mesopotamian literature, the epic poem of Gilgamesh has allusions to Odonata which signify the impossibility of immortality. Amongst the Aborigines of Australia of the Arrernte language groups, honey ants and witchety grubs served as personal clan totems. In the case of the 'San' bush-men of the Kalahari, it is the praying mantis which holds much cultural significance including creation and zen-like patience in waiting.:9\\n'''\\n\\nQuestion: What kind of ants are symbolic among the Australian Aborigines?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\",\n",
       "  'response': ['The praying mantis.',\n",
       "   'According to the context, among the Australian Aborigines, honey ants are symbolic of personal clan totems.',\n",
       "   'The answer is honey ants.',\n",
       "   'The praying mantis is symbolic among the Australian Aborigines of the Arrernte language groups.']}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab102989",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "responses = json_data[index][\"response\"]\n",
    "\n",
    "question = json_data[index][\"additional_info\"][\"question\"]\n",
    "context = json_data[index][\"additional_info\"][\"context\"]\n",
    "answer = json_data[index][\"additional_info\"][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a32454b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The answer is books',\n",
       " 'According to the context, Ann Arbor ranks 1st among booksellers.',\n",
       " 'The answer is books',\n",
       " 'Based on the context, Ann Arbor ranks 1st among books sold.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8831411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response[0] = 'According to the text, 84% of Chinese polled supported the death penalty for those leaving Japan.'\n",
    "# response[1] = 'According to the text, 84% of Egyptians polled support the death penalty for those leaving Japan.'\n",
    "# # response[2] = 'The answer is 25%.'\n",
    "# response[3] = 'The text does not specify the percentage, thus there is no answer. However, it does mention that 84% of Egyptians polled supported the death penalty for those who leave Islam.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cfabb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a meticulous AI quality assurance expert and diagnostician. Your sole function is to evaluate a given response and provide a structured diagnosis for every flaw you find. You MUST NOT use any external knowledge and base your analysis *only* on the provided context and correct answer.\n",
      "\n",
      "# Inputs\n",
      "**Question**: Ann Arbor ranks 1st among what goods sold?\n",
      "**Context**: '''The Ann Arbor Hands-On Museum is located in a renovated and expanded historic downtown fire station. Multiple art galleries exist in the city, notably in the downtown area and around the University of Michigan campus. Aside from a large restaurant scene in the Main Street, South State Street, and South University Avenue areas, Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The Ann Arbor District Library maintains four branch outlets in addition to its main downtown building. The city is also home to the Gerald R. Ford Presidential Library.'''\n",
      "**Correct Answer**: ['books']\n",
      "**Response to Evaluate**: The answer is books\n",
      "\n",
      "# Task:\n",
      "Your mission is to meticulously analyze the `Response to Evaluate` and identify all its flaws. For each flaw, you must produce a structured error report with three parts: a description of the error, its exact location in the text, and a precise, actionable correction plan.\n",
      "\n",
      "# Error Structure Explained:\n",
      "For each error found, you must provide:\n",
      "1. `error`: A concrete and concise description of the flaw.\n",
      "2. `error_location`: The exact substring/phrase/word from the `Response to Evaluate` that is incorrect. This should be as specific as possible.\n",
      "3. `error_correction`: A clear, actionable plan to fix the error. This plan MUST:\n",
      "   - Specify the exact part of the text to be **deleted** (if any). Focus on the smallest possible change.\n",
      "   - Adhere to Word Integrity: Do not delete sub-words (like suffixes or prefixes), delete the whole word instead.\n",
      "   - If you apply deletion, the minimum change you can make is to delete the word (not a token) that is incorrect, you cannot delete just a punctuation character like '.' or ','.\n",
      "   - Specify the exact text to be **added** (if any).\n",
      "   - Consider grammatical consequences. For instance, if deleting 'apples' from 'apples and oranges' leaves a dangling 'and', your plan must state that both 'apples' and 'and' should be deleted to maintain a grammatically correct sentence.\n",
      "   - Ensure that following the correction plan, correct answer will be obtained.\n",
      "   - Always focus on the most effective way to fix the error, one that requires the fewest edits (deletions and additions) possible. You MUST provide only one, most efficient correction plan and not multiple alternative solutions.\n",
      "\n",
      "# Step-by-Step Analysis Process:\n",
      "1. **Identify a Flaw**: Compare the `Response to Evaluate` against the `Correct Answer` and `Context`:\n",
      "   - Does the response correctly answer the question when compared to the `Correct Answer`?\n",
      "   - Is all information in the response supported by the `Context`?\n",
      "   - Is there any other issue, like self-contradiction or grammarical errors?\n",
      "**Crucial Rule**: Your evaluation must focus strictly on the factual and semantic correctness of the answer. You MUST NOT flag a response for containing harmless, non-essential information if the core answer is correct. A response is still considered CORRECT even if it includes:\n",
      "   - Introductory phrases: such as \"According to the text...\" or \"The context states...\"\n",
      "   - Benign conversational text or formatting: such as markdown, greetings, or other chat-like filler\n",
      "   - Minor, non-contradictory details: extra information pulled from the Context that does not alter or conflict with the core meaning of the Correct Answer\n",
      "Flag an error only when the response becomes factually incorrect, semantically different from the Correct Answer, or introduces a contradiction. Do not penalize for style or verbosity.\n",
      "\n",
      "2. **Pinpoint the Location**: Isolate the exact segment of text in the `Response to Evaluate` where the error resides. This will be your `error_location`.\n",
      "3. **Formulate a Correction Plan**: Devise the most efficient way to fix the error. What is the minimum text to delete and add? Does your change break the sentence's grammar? If so, expand your plan to include any necessary grammatical cleanup. This becomes your `error_correction`.\n",
      "4. **Isolate Unique Errors**: Each error you report must be distinct and map to a unique segment of the Response to Evaluate. You cannot report two different errors for the exact same word or phrase. Ensure your error reports cover different, non-overlapping parts of the text.\n",
      "5. **Compile and Repeat**: Combine these three pieces of information into a structured error. Each distinct error must be listed only once. Then, continue analyzing the rest of the response for more errors.\n",
      "\n",
      "# Output Requirements\n",
      "1. You MUST produce a list of structured errors adhering to the format, ensuring each error appears only once.\n",
      "2. If the response is completely correct and faithful to the context, you MUST return an empty list: `[]`.\n",
      "3. You MUST strictly adhere to the provided format instructions.\n",
      "4. Do NOT add any headers, comments, or conversational text to your final output. Your entire response should be only the list of structured errors.\n",
      "\n",
      "# Format Instructions\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Error\": {\"description\": \"A model to represent an error found in a response.\", \"properties\": {\"error_description\": {\"description\": \"The error found in the response.\", \"title\": \"Error Description\", \"type\": \"string\"}, \"error_location\": {\"description\": \"The location of the error in the response.\", \"title\": \"Error Location\", \"type\": \"string\"}, \"error_correction\": {\"description\": \"Explanation on how to correct the error.\", \"title\": \"Error Correction\", \"type\": \"string\"}}, \"required\": [\"error_description\", \"error_location\", \"error_correction\"], \"title\": \"Error\", \"type\": \"object\"}}, \"description\": \"A model to hold a list of errors found in a response.\", \"properties\": {\"errors\": {\"default\": [], \"description\": \"A list of errors found in the response.\", \"items\": {\"$ref\": \"#/$defs/Error\"}, \"title\": \"Errors\", \"type\": \"array\"}}}\n",
      "```\n",
      "\n",
      "List of Errors:\n",
      "You are a meticulous AI quality assurance expert and diagnostician. Your sole function is to evaluate a given response and provide a structured diagnosis for every flaw you find. You MUST NOT use any external knowledge and base your analysis *only* on the provided context and correct answer.\n",
      "\n",
      "# Inputs\n",
      "**Question**: Ann Arbor ranks 1st among what goods sold?\n",
      "**Context**: '''The Ann Arbor Hands-On Museum is located in a renovated and expanded historic downtown fire station. Multiple art galleries exist in the city, notably in the downtown area and around the University of Michigan campus. Aside from a large restaurant scene in the Main Street, South State Street, and South University Avenue areas, Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The Ann Arbor District Library maintains four branch outlets in addition to its main downtown building. The city is also home to the Gerald R. Ford Presidential Library.'''\n",
      "**Correct Answer**: ['books']\n",
      "**Response to Evaluate**: According to the context, Ann Arbor ranks 1st among booksellers.\n",
      "\n",
      "# Task:\n",
      "Your mission is to meticulously analyze the `Response to Evaluate` and identify all its flaws. For each flaw, you must produce a structured error report with three parts: a description of the error, its exact location in the text, and a precise, actionable correction plan.\n",
      "\n",
      "# Error Structure Explained:\n",
      "For each error found, you must provide:\n",
      "1. `error`: A concrete and concise description of the flaw.\n",
      "2. `error_location`: The exact substring/phrase/word from the `Response to Evaluate` that is incorrect. This should be as specific as possible.\n",
      "3. `error_correction`: A clear, actionable plan to fix the error. This plan MUST:\n",
      "   - Specify the exact part of the text to be **deleted** (if any). Focus on the smallest possible change.\n",
      "   - Adhere to Word Integrity: Do not delete sub-words (like suffixes or prefixes), delete the whole word instead.\n",
      "   - If you apply deletion, the minimum change you can make is to delete the word (not a token) that is incorrect, you cannot delete just a punctuation character like '.' or ','.\n",
      "   - Specify the exact text to be **added** (if any).\n",
      "   - Consider grammatical consequences. For instance, if deleting 'apples' from 'apples and oranges' leaves a dangling 'and', your plan must state that both 'apples' and 'and' should be deleted to maintain a grammatically correct sentence.\n",
      "   - Ensure that following the correction plan, correct answer will be obtained.\n",
      "   - Always focus on the most effective way to fix the error, one that requires the fewest edits (deletions and additions) possible. You MUST provide only one, most efficient correction plan and not multiple alternative solutions.\n",
      "\n",
      "# Step-by-Step Analysis Process:\n",
      "1. **Identify a Flaw**: Compare the `Response to Evaluate` against the `Correct Answer` and `Context`:\n",
      "   - Does the response correctly answer the question when compared to the `Correct Answer`?\n",
      "   - Is all information in the response supported by the `Context`?\n",
      "   - Is there any other issue, like self-contradiction or grammarical errors?\n",
      "**Crucial Rule**: Your evaluation must focus strictly on the factual and semantic correctness of the answer. You MUST NOT flag a response for containing harmless, non-essential information if the core answer is correct. A response is still considered CORRECT even if it includes:\n",
      "   - Introductory phrases: such as \"According to the text...\" or \"The context states...\"\n",
      "   - Benign conversational text or formatting: such as markdown, greetings, or other chat-like filler\n",
      "   - Minor, non-contradictory details: extra information pulled from the Context that does not alter or conflict with the core meaning of the Correct Answer\n",
      "Flag an error only when the response becomes factually incorrect, semantically different from the Correct Answer, or introduces a contradiction. Do not penalize for style or verbosity.\n",
      "\n",
      "2. **Pinpoint the Location**: Isolate the exact segment of text in the `Response to Evaluate` where the error resides. This will be your `error_location`.\n",
      "3. **Formulate a Correction Plan**: Devise the most efficient way to fix the error. What is the minimum text to delete and add? Does your change break the sentence's grammar? If so, expand your plan to include any necessary grammatical cleanup. This becomes your `error_correction`.\n",
      "4. **Isolate Unique Errors**: Each error you report must be distinct and map to a unique segment of the Response to Evaluate. You cannot report two different errors for the exact same word or phrase. Ensure your error reports cover different, non-overlapping parts of the text.\n",
      "5. **Compile and Repeat**: Combine these three pieces of information into a structured error. Each distinct error must be listed only once. Then, continue analyzing the rest of the response for more errors.\n",
      "\n",
      "# Output Requirements\n",
      "1. You MUST produce a list of structured errors adhering to the format, ensuring each error appears only once.\n",
      "2. If the response is completely correct and faithful to the context, you MUST return an empty list: `[]`.\n",
      "3. You MUST strictly adhere to the provided format instructions.\n",
      "4. Do NOT add any headers, comments, or conversational text to your final output. Your entire response should be only the list of structured errors.\n",
      "\n",
      "# Format Instructions\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Error\": {\"description\": \"A model to represent an error found in a response.\", \"properties\": {\"error_description\": {\"description\": \"The error found in the response.\", \"title\": \"Error Description\", \"type\": \"string\"}, \"error_location\": {\"description\": \"The location of the error in the response.\", \"title\": \"Error Location\", \"type\": \"string\"}, \"error_correction\": {\"description\": \"Explanation on how to correct the error.\", \"title\": \"Error Correction\", \"type\": \"string\"}}, \"required\": [\"error_description\", \"error_location\", \"error_correction\"], \"title\": \"Error\", \"type\": \"object\"}}, \"description\": \"A model to hold a list of errors found in a response.\", \"properties\": {\"errors\": {\"default\": [], \"description\": \"A list of errors found in the response.\", \"items\": {\"$ref\": \"#/$defs/Error\"}, \"title\": \"Errors\", \"type\": \"array\"}}}\n",
      "```\n",
      "\n",
      "List of Errors:\n",
      "You are a meticulous AI quality assurance expert and diagnostician. Your sole function is to evaluate a given response and provide a structured diagnosis for every flaw you find. You MUST NOT use any external knowledge and base your analysis *only* on the provided context and correct answer.\n",
      "\n",
      "# Inputs\n",
      "**Question**: Ann Arbor ranks 1st among what goods sold?\n",
      "**Context**: '''The Ann Arbor Hands-On Museum is located in a renovated and expanded historic downtown fire station. Multiple art galleries exist in the city, notably in the downtown area and around the University of Michigan campus. Aside from a large restaurant scene in the Main Street, South State Street, and South University Avenue areas, Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The Ann Arbor District Library maintains four branch outlets in addition to its main downtown building. The city is also home to the Gerald R. Ford Presidential Library.'''\n",
      "**Correct Answer**: ['books']\n",
      "**Response to Evaluate**: The answer is books\n",
      "\n",
      "# Task:\n",
      "Your mission is to meticulously analyze the `Response to Evaluate` and identify all its flaws. For each flaw, you must produce a structured error report with three parts: a description of the error, its exact location in the text, and a precise, actionable correction plan.\n",
      "\n",
      "# Error Structure Explained:\n",
      "For each error found, you must provide:\n",
      "1. `error`: A concrete and concise description of the flaw.\n",
      "2. `error_location`: The exact substring/phrase/word from the `Response to Evaluate` that is incorrect. This should be as specific as possible.\n",
      "3. `error_correction`: A clear, actionable plan to fix the error. This plan MUST:\n",
      "   - Specify the exact part of the text to be **deleted** (if any). Focus on the smallest possible change.\n",
      "   - Adhere to Word Integrity: Do not delete sub-words (like suffixes or prefixes), delete the whole word instead.\n",
      "   - If you apply deletion, the minimum change you can make is to delete the word (not a token) that is incorrect, you cannot delete just a punctuation character like '.' or ','.\n",
      "   - Specify the exact text to be **added** (if any).\n",
      "   - Consider grammatical consequences. For instance, if deleting 'apples' from 'apples and oranges' leaves a dangling 'and', your plan must state that both 'apples' and 'and' should be deleted to maintain a grammatically correct sentence.\n",
      "   - Ensure that following the correction plan, correct answer will be obtained.\n",
      "   - Always focus on the most effective way to fix the error, one that requires the fewest edits (deletions and additions) possible. You MUST provide only one, most efficient correction plan and not multiple alternative solutions.\n",
      "\n",
      "# Step-by-Step Analysis Process:\n",
      "1. **Identify a Flaw**: Compare the `Response to Evaluate` against the `Correct Answer` and `Context`:\n",
      "   - Does the response correctly answer the question when compared to the `Correct Answer`?\n",
      "   - Is all information in the response supported by the `Context`?\n",
      "   - Is there any other issue, like self-contradiction or grammarical errors?\n",
      "**Crucial Rule**: Your evaluation must focus strictly on the factual and semantic correctness of the answer. You MUST NOT flag a response for containing harmless, non-essential information if the core answer is correct. A response is still considered CORRECT even if it includes:\n",
      "   - Introductory phrases: such as \"According to the text...\" or \"The context states...\"\n",
      "   - Benign conversational text or formatting: such as markdown, greetings, or other chat-like filler\n",
      "   - Minor, non-contradictory details: extra information pulled from the Context that does not alter or conflict with the core meaning of the Correct Answer\n",
      "Flag an error only when the response becomes factually incorrect, semantically different from the Correct Answer, or introduces a contradiction. Do not penalize for style or verbosity.\n",
      "\n",
      "2. **Pinpoint the Location**: Isolate the exact segment of text in the `Response to Evaluate` where the error resides. This will be your `error_location`.\n",
      "3. **Formulate a Correction Plan**: Devise the most efficient way to fix the error. What is the minimum text to delete and add? Does your change break the sentence's grammar? If so, expand your plan to include any necessary grammatical cleanup. This becomes your `error_correction`.\n",
      "4. **Isolate Unique Errors**: Each error you report must be distinct and map to a unique segment of the Response to Evaluate. You cannot report two different errors for the exact same word or phrase. Ensure your error reports cover different, non-overlapping parts of the text.\n",
      "5. **Compile and Repeat**: Combine these three pieces of information into a structured error. Each distinct error must be listed only once. Then, continue analyzing the rest of the response for more errors.\n",
      "\n",
      "# Output Requirements\n",
      "1. You MUST produce a list of structured errors adhering to the format, ensuring each error appears only once.\n",
      "2. If the response is completely correct and faithful to the context, you MUST return an empty list: `[]`.\n",
      "3. You MUST strictly adhere to the provided format instructions.\n",
      "4. Do NOT add any headers, comments, or conversational text to your final output. Your entire response should be only the list of structured errors.\n",
      "\n",
      "# Format Instructions\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Error\": {\"description\": \"A model to represent an error found in a response.\", \"properties\": {\"error_description\": {\"description\": \"The error found in the response.\", \"title\": \"Error Description\", \"type\": \"string\"}, \"error_location\": {\"description\": \"The location of the error in the response.\", \"title\": \"Error Location\", \"type\": \"string\"}, \"error_correction\": {\"description\": \"Explanation on how to correct the error.\", \"title\": \"Error Correction\", \"type\": \"string\"}}, \"required\": [\"error_description\", \"error_location\", \"error_correction\"], \"title\": \"Error\", \"type\": \"object\"}}, \"description\": \"A model to hold a list of errors found in a response.\", \"properties\": {\"errors\": {\"default\": [], \"description\": \"A list of errors found in the response.\", \"items\": {\"$ref\": \"#/$defs/Error\"}, \"title\": \"Errors\", \"type\": \"array\"}}}\n",
      "```\n",
      "\n",
      "List of Errors:\n",
      "You are a meticulous AI quality assurance expert and diagnostician. Your sole function is to evaluate a given response and provide a structured diagnosis for every flaw you find. You MUST NOT use any external knowledge and base your analysis *only* on the provided context and correct answer.\n",
      "\n",
      "# Inputs\n",
      "**Question**: Ann Arbor ranks 1st among what goods sold?\n",
      "**Context**: '''The Ann Arbor Hands-On Museum is located in a renovated and expanded historic downtown fire station. Multiple art galleries exist in the city, notably in the downtown area and around the University of Michigan campus. Aside from a large restaurant scene in the Main Street, South State Street, and South University Avenue areas, Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The Ann Arbor District Library maintains four branch outlets in addition to its main downtown building. The city is also home to the Gerald R. Ford Presidential Library.'''\n",
      "**Correct Answer**: ['books']\n",
      "**Response to Evaluate**: Based on the context, Ann Arbor ranks 1st among books sold.\n",
      "\n",
      "# Task:\n",
      "Your mission is to meticulously analyze the `Response to Evaluate` and identify all its flaws. For each flaw, you must produce a structured error report with three parts: a description of the error, its exact location in the text, and a precise, actionable correction plan.\n",
      "\n",
      "# Error Structure Explained:\n",
      "For each error found, you must provide:\n",
      "1. `error`: A concrete and concise description of the flaw.\n",
      "2. `error_location`: The exact substring/phrase/word from the `Response to Evaluate` that is incorrect. This should be as specific as possible.\n",
      "3. `error_correction`: A clear, actionable plan to fix the error. This plan MUST:\n",
      "   - Specify the exact part of the text to be **deleted** (if any). Focus on the smallest possible change.\n",
      "   - Adhere to Word Integrity: Do not delete sub-words (like suffixes or prefixes), delete the whole word instead.\n",
      "   - If you apply deletion, the minimum change you can make is to delete the word (not a token) that is incorrect, you cannot delete just a punctuation character like '.' or ','.\n",
      "   - Specify the exact text to be **added** (if any).\n",
      "   - Consider grammatical consequences. For instance, if deleting 'apples' from 'apples and oranges' leaves a dangling 'and', your plan must state that both 'apples' and 'and' should be deleted to maintain a grammatically correct sentence.\n",
      "   - Ensure that following the correction plan, correct answer will be obtained.\n",
      "   - Always focus on the most effective way to fix the error, one that requires the fewest edits (deletions and additions) possible. You MUST provide only one, most efficient correction plan and not multiple alternative solutions.\n",
      "\n",
      "# Step-by-Step Analysis Process:\n",
      "1. **Identify a Flaw**: Compare the `Response to Evaluate` against the `Correct Answer` and `Context`:\n",
      "   - Does the response correctly answer the question when compared to the `Correct Answer`?\n",
      "   - Is all information in the response supported by the `Context`?\n",
      "   - Is there any other issue, like self-contradiction or grammarical errors?\n",
      "**Crucial Rule**: Your evaluation must focus strictly on the factual and semantic correctness of the answer. You MUST NOT flag a response for containing harmless, non-essential information if the core answer is correct. A response is still considered CORRECT even if it includes:\n",
      "   - Introductory phrases: such as \"According to the text...\" or \"The context states...\"\n",
      "   - Benign conversational text or formatting: such as markdown, greetings, or other chat-like filler\n",
      "   - Minor, non-contradictory details: extra information pulled from the Context that does not alter or conflict with the core meaning of the Correct Answer\n",
      "Flag an error only when the response becomes factually incorrect, semantically different from the Correct Answer, or introduces a contradiction. Do not penalize for style or verbosity.\n",
      "\n",
      "2. **Pinpoint the Location**: Isolate the exact segment of text in the `Response to Evaluate` where the error resides. This will be your `error_location`.\n",
      "3. **Formulate a Correction Plan**: Devise the most efficient way to fix the error. What is the minimum text to delete and add? Does your change break the sentence's grammar? If so, expand your plan to include any necessary grammatical cleanup. This becomes your `error_correction`.\n",
      "4. **Isolate Unique Errors**: Each error you report must be distinct and map to a unique segment of the Response to Evaluate. You cannot report two different errors for the exact same word or phrase. Ensure your error reports cover different, non-overlapping parts of the text.\n",
      "5. **Compile and Repeat**: Combine these three pieces of information into a structured error. Each distinct error must be listed only once. Then, continue analyzing the rest of the response for more errors.\n",
      "\n",
      "# Output Requirements\n",
      "1. You MUST produce a list of structured errors adhering to the format, ensuring each error appears only once.\n",
      "2. If the response is completely correct and faithful to the context, you MUST return an empty list: `[]`.\n",
      "3. You MUST strictly adhere to the provided format instructions.\n",
      "4. Do NOT add any headers, comments, or conversational text to your final output. Your entire response should be only the list of structured errors.\n",
      "\n",
      "# Format Instructions\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Error\": {\"description\": \"A model to represent an error found in a response.\", \"properties\": {\"error_description\": {\"description\": \"The error found in the response.\", \"title\": \"Error Description\", \"type\": \"string\"}, \"error_location\": {\"description\": \"The location of the error in the response.\", \"title\": \"Error Location\", \"type\": \"string\"}, \"error_correction\": {\"description\": \"Explanation on how to correct the error.\", \"title\": \"Error Correction\", \"type\": \"string\"}}, \"required\": [\"error_description\", \"error_location\", \"error_correction\"], \"title\": \"Error\", \"type\": \"object\"}}, \"description\": \"A model to hold a list of errors found in a response.\", \"properties\": {\"errors\": {\"default\": [], \"description\": \"A list of errors found in the response.\", \"items\": {\"$ref\": \"#/$defs/Error\"}, \"title\": \"Errors\", \"type\": \"array\"}}}\n",
      "```\n",
      "\n",
      "List of Errors:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-22 20:30:52.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.context_qa_correction_agent\u001b[0m:\u001b[36mcheck_for_errors\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mErrors: [[], [Error(error_description='The response states that Ann Arbor ranks 1st among booksellers, but the context states that Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The correct answer is books.', error_location='booksellers', error_correction='Delete \"booksellers\" and add \"books\".')], [], [Error(error_description='The response is not concise and contains unnecessary words.', error_location='Based on the context, Ann Arbor ranks 1st among', error_correction=\"Delete 'Based on the context, '\"), Error(error_description=\"The answer should be 'books' instead of 'books sold'.\", error_location='books sold', error_correction=\"Delete 'sold'\")]]\u001b[0m\n",
      "\u001b[32m2025-07-22 20:30:52.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.context_qa_correction_agent\u001b[0m:\u001b[36mshould_correct\u001b[0m:\u001b[36m101\u001b[0m - \u001b[1mError number: 2\u001b[0m\n",
      "\u001b[32m2025-07-22 20:30:52.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.context_qa_correction_agent\u001b[0m:\u001b[36mget_errors_to_correct\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1mErrors to correct: ([Error(error_description='The response is not concise and contains unnecessary words.', error_location='Based on the context, Ann Arbor ranks 1st among', error_correction=\"Delete 'Based on the context, '\"), Error(error_description=\"The answer should be 'books' instead of 'books sold'.\", error_location='books sold', error_correction=\"Delete 'sold'\")], [Error(error_description='The response states that Ann Arbor ranks 1st among booksellers, but the context states that Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The correct answer is books.', error_location='booksellers', error_correction='Delete \"booksellers\" and add \"books\".')])\u001b[0m\n",
      "\u001b[32m2025-07-22 20:30:52.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.context_qa_correction_agent\u001b[0m:\u001b[36mget_errors_to_correct\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mResponses to correct: ('Based on the context, Ann Arbor ranks 1st among books sold.', 'According to the context, Ann Arbor ranks 1st among booksellers.')\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a Master Editor. Your task is to perform a final, definitive correction on a flawed response using a special token-based editing language. You will act as the final authority, using a junior editor's analysis as guidance but not as a command.\n",
      "\n",
      "# Core Task\n",
      "You will be given the `Question`, `Context`, `Correct Answer`, `Incorrect Response`, and `Errors in response`. Your mission is to apply precise edits to the `Incorrect Response` to make it grammatically perfect and semantically identical to the `Correct Answer`.\n",
      "\n",
      "# Token-Based Editing Language\n",
      "You have two operations at your disposal: deleting text with special tokens and adding new text. You may use one or both.\n",
      "\n",
      "1.  **<DEL_W> (Delete Word)**: Deletes the single word (not a token!) immediately preceding it. A word is a string of characters separated by spaces. Punctuation is part of the word (e.g., 'end.').\n",
      "    *   **Pro Tip**: Deleting a single word can create a grammatical flaw (e.g., a dangling 'and'). Always check the context and be prepared to delete adjacent words to maintain a grammatically perfect sentence.\n",
      "\n",
      "2.  **<DEL_S> (Delete Sentence)**: Deletes text from its position back to the beginning of the **current sentence**.\n",
      "    *   **Full Sentence Deletion**: When placed at the very end of a sentence (after the punctuation), it deletes the entire sentence.\n",
      "    *   **Prefix Deletion**: When placed in the middle of a sentence, it efficiently deletes just the beginning part of that sentence, which is more efficient than using multiple `<DEL_W>` tokens.\n",
      "\n",
      "3.  **<DEL_A> (Delete All)**: Deletes all text in the response that comes before it, up to the very beginning of the **entire response**.\n",
      "    *   **Pro Tip**: This is your most powerful tool. It is perfect for complete rewrites or for removing multiple sentences and prefixes at once.\n",
      "\n",
      "# Your Workflow\n",
      "1.  **Review the Analysis**: Carefully examine the `Errors in response`. Use this junior editor's analysis to quickly understand the core issues.\n",
      "2.  **Formulate Your Master Plan**: Do not blindly follow the suggestions. Use your expert judgment to devise the most efficient correction. The best correction uses the fewest tokens and results in a grammatically flawless sentence that matches the `Correct Answer`.\n",
      "3.  **Execute the Edit**: Apply your plan to the `Incorrect Response`. This is a direct edit, not a rewrite. You will insert the tokens and any new text directly into the original string.\n",
      "4.  **Final Output**: Ensure your response contains *only* the final, corrected text. No extra headers, explanations, or conversational text.\n",
      "\n",
      "# Correction Masterclass (Examples)\n",
      "## Example 1: Simple Word Replacement\n",
      "**Question**: What color is the sky on a clear day?\n",
      "**Context**: The sky is blue due to Rayleigh scattering of sunlight in the atmosphere.\n",
      "**Correct Answer**: The sky is blue.\n",
      "**Incorrect Response**: The sky is green.\n",
      "**Errors in response**:\n",
      "Error: Incorrect color\n",
      "Error Location: green\n",
      "Error Correction: Delete 'green' and add 'blue'.\n",
      "**Corrected Response**: The sky is green<DEL_W> blue.\n",
      "\n",
      "## Example 2: Deleting an Unnecessary Word\n",
      "**Question**: What is the capital of France?\n",
      "**Context**: Paris is the capital of France.\n",
      "**Correct Answer**: Paris is the capital of France.\n",
      "**Incorrect Response**: Paris is the beautiful capital of France.\n",
      "**Errors in response**:\n",
      "Error: Unnecessary adjective\n",
      "Error Location: beautiful\n",
      "Error Correction: Delete the word 'beautiful'.\n",
      "**Corrected Response**: Paris is the beautiful<DEL_W> capital of France.\n",
      "\n",
      "## Example 3: Correcting an Incomplete Suggestion\n",
      "**Question**: What did Kate sell on the market?\n",
      "**Context**: The report says Kate sold many tomatoes on the market.\n",
      "**Correct Answer**: Kate sold many tomatoes on the market.\n",
      "**Incorrect Response**: Kate sold many potatoes and tomatoes on the market.\n",
      "**Errors in response**:\n",
      "Error: Incorrect item listed\n",
      "Error Location: potatoes\n",
      "Error Correction: Delete 'potatoes and'.\n",
      "**Corrected Response**: Kate sold many potatoes<DEL_W> and<DEL_W> tomatoes on the market.\n",
      "\n",
      "## Example 4: Fixing a Sentence\n",
      "**Question**: What are the first three planets from the Sun, and what is a key feature of the third one?\n",
      "**Context**: The order of planets is Mercury, Venus, Earth... Earth, the third planet, is the only planet in our solar system known to harbor life.\n",
      "**Correct Answer**: The first three planets from the Sun are Mercury, Venus, and Earth. Earth is the only planet in our solar system known to harbor life.\n",
      "**Incorrect Response**: The first three planets from the Sun are Mercury, Venus, and Earth. Earth is the largest planet in our solar system.\n",
      "**Errors in response**:\n",
      "Error: Incorrect fact about Earth\n",
      "Error Location: Earth is the largest planet.\n",
      "Error Correction: Delete the second sentence.\n",
      "**Corrected Response**: The first three planets from the Sun are Mercury, Venus, and Earth. Earth is the largest planet in our solar system.<DEL_S> Earth is the only planet in our solar system known to harbor life.\n",
      "\n",
      "## Example 5: Using <DEL_S> to delete the first part of the sentence\n",
      "**Question**: How many people left the room during the first 10 minutes?\n",
      "**Context**: After 10 minutes, 5 people left the room.\n",
      "**Correct Answer**: 5 people\n",
      "**Incorrect Response**: The context does not provide a clear answer, yet it states that 5 people left the room.\n",
      "**Errors in response**:\n",
      "Error: The response contradicts itself and initially provide wrong information\n",
      "Error Location: The context does not provide a clear answer, yet it states that\n",
      "Error Correction: Delete 'The context does not provide a clear answer, yet it states that'\n",
      "**Corrected Response**: The context does not provide a clear answer, yet it states that<DEL_S> 5 people left the room.\n",
      "\n",
      "## Example 6: Complete Rewrite\n",
      "**Question**: What is the primary cause of Earth's seasons?\n",
      "**Context**: The Earth's tilt on its axis is the primary cause of the seasons.\n",
      "**Correct Answer**: The tilt of the Earth's axis causes the seasons.\n",
      "**Incorrect Response**: The distance from the sun causes the seasons. It gets warmer when we are closer to the sun.\n",
      "**Errors in response**:\n",
      "Error: Fundamentally incorrect reasoning\n",
      "Error Location: The distance from the sun causes the seasons. It gets warmer when we are closer to the sun.\n",
      "Error Correction: Delete the entire response and replace it with the correct answer.\n",
      "**Corrected Response**: The distance from the sun causes the seasons. It gets warmer when we are closer to the sun.<DEL_A> The tilt of the Earth's axis causes the seasons.\n",
      "\n",
      "# Your Turn:\n",
      "**Question**: Ann Arbor ranks 1st among what goods sold?\n",
      "**Context**: '''The Ann Arbor Hands-On Museum is located in a renovated and expanded historic downtown fire station. Multiple art galleries exist in the city, notably in the downtown area and around the University of Michigan campus. Aside from a large restaurant scene in the Main Street, South State Street, and South University Avenue areas, Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The Ann Arbor District Library maintains four branch outlets in addition to its main downtown building. The city is also home to the Gerald R. Ford Presidential Library.'''\n",
      "**Correct Answer**: ['books']\n",
      "**Incorrect Response**: Based on the context, Ann Arbor ranks 1st among books sold.\n",
      "**Errors in response**:\n",
      "Error: The response is not concise and contains unnecessary words.\n",
      "Error Location: Based on the context, Ann Arbor ranks 1st among\n",
      "How to fix the error: Delete 'Based on the context, '\n",
      "\n",
      "\n",
      "Error: The answer should be 'books' instead of 'books sold'.\n",
      "Error Location: books sold\n",
      "How to fix the error: Delete 'sold'\n",
      "\n",
      "**Corrected Response**:\n",
      "You are a Master Editor. Your task is to perform a final, definitive correction on a flawed response using a special token-based editing language. You will act as the final authority, using a junior editor's analysis as guidance but not as a command.\n",
      "\n",
      "# Core Task\n",
      "You will be given the `Question`, `Context`, `Correct Answer`, `Incorrect Response`, and `Errors in response`. Your mission is to apply precise edits to the `Incorrect Response` to make it grammatically perfect and semantically identical to the `Correct Answer`.\n",
      "\n",
      "# Token-Based Editing Language\n",
      "You have two operations at your disposal: deleting text with special tokens and adding new text. You may use one or both.\n",
      "\n",
      "1.  **<DEL_W> (Delete Word)**: Deletes the single word (not a token!) immediately preceding it. A word is a string of characters separated by spaces. Punctuation is part of the word (e.g., 'end.').\n",
      "    *   **Pro Tip**: Deleting a single word can create a grammatical flaw (e.g., a dangling 'and'). Always check the context and be prepared to delete adjacent words to maintain a grammatically perfect sentence.\n",
      "\n",
      "2.  **<DEL_S> (Delete Sentence)**: Deletes text from its position back to the beginning of the **current sentence**.\n",
      "    *   **Full Sentence Deletion**: When placed at the very end of a sentence (after the punctuation), it deletes the entire sentence.\n",
      "    *   **Prefix Deletion**: When placed in the middle of a sentence, it efficiently deletes just the beginning part of that sentence, which is more efficient than using multiple `<DEL_W>` tokens.\n",
      "\n",
      "3.  **<DEL_A> (Delete All)**: Deletes all text in the response that comes before it, up to the very beginning of the **entire response**.\n",
      "    *   **Pro Tip**: This is your most powerful tool. It is perfect for complete rewrites or for removing multiple sentences and prefixes at once.\n",
      "\n",
      "# Your Workflow\n",
      "1.  **Review the Analysis**: Carefully examine the `Errors in response`. Use this junior editor's analysis to quickly understand the core issues.\n",
      "2.  **Formulate Your Master Plan**: Do not blindly follow the suggestions. Use your expert judgment to devise the most efficient correction. The best correction uses the fewest tokens and results in a grammatically flawless sentence that matches the `Correct Answer`.\n",
      "3.  **Execute the Edit**: Apply your plan to the `Incorrect Response`. This is a direct edit, not a rewrite. You will insert the tokens and any new text directly into the original string.\n",
      "4.  **Final Output**: Ensure your response contains *only* the final, corrected text. No extra headers, explanations, or conversational text.\n",
      "\n",
      "# Correction Masterclass (Examples)\n",
      "## Example 1: Simple Word Replacement\n",
      "**Question**: What color is the sky on a clear day?\n",
      "**Context**: The sky is blue due to Rayleigh scattering of sunlight in the atmosphere.\n",
      "**Correct Answer**: The sky is blue.\n",
      "**Incorrect Response**: The sky is green.\n",
      "**Errors in response**:\n",
      "Error: Incorrect color\n",
      "Error Location: green\n",
      "Error Correction: Delete 'green' and add 'blue'.\n",
      "**Corrected Response**: The sky is green<DEL_W> blue.\n",
      "\n",
      "## Example 2: Deleting an Unnecessary Word\n",
      "**Question**: What is the capital of France?\n",
      "**Context**: Paris is the capital of France.\n",
      "**Correct Answer**: Paris is the capital of France.\n",
      "**Incorrect Response**: Paris is the beautiful capital of France.\n",
      "**Errors in response**:\n",
      "Error: Unnecessary adjective\n",
      "Error Location: beautiful\n",
      "Error Correction: Delete the word 'beautiful'.\n",
      "**Corrected Response**: Paris is the beautiful<DEL_W> capital of France.\n",
      "\n",
      "## Example 3: Correcting an Incomplete Suggestion\n",
      "**Question**: What did Kate sell on the market?\n",
      "**Context**: The report says Kate sold many tomatoes on the market.\n",
      "**Correct Answer**: Kate sold many tomatoes on the market.\n",
      "**Incorrect Response**: Kate sold many potatoes and tomatoes on the market.\n",
      "**Errors in response**:\n",
      "Error: Incorrect item listed\n",
      "Error Location: potatoes\n",
      "Error Correction: Delete 'potatoes and'.\n",
      "**Corrected Response**: Kate sold many potatoes<DEL_W> and<DEL_W> tomatoes on the market.\n",
      "\n",
      "## Example 4: Fixing a Sentence\n",
      "**Question**: What are the first three planets from the Sun, and what is a key feature of the third one?\n",
      "**Context**: The order of planets is Mercury, Venus, Earth... Earth, the third planet, is the only planet in our solar system known to harbor life.\n",
      "**Correct Answer**: The first three planets from the Sun are Mercury, Venus, and Earth. Earth is the only planet in our solar system known to harbor life.\n",
      "**Incorrect Response**: The first three planets from the Sun are Mercury, Venus, and Earth. Earth is the largest planet in our solar system.\n",
      "**Errors in response**:\n",
      "Error: Incorrect fact about Earth\n",
      "Error Location: Earth is the largest planet.\n",
      "Error Correction: Delete the second sentence.\n",
      "**Corrected Response**: The first three planets from the Sun are Mercury, Venus, and Earth. Earth is the largest planet in our solar system.<DEL_S> Earth is the only planet in our solar system known to harbor life.\n",
      "\n",
      "## Example 5: Using <DEL_S> to delete the first part of the sentence\n",
      "**Question**: How many people left the room during the first 10 minutes?\n",
      "**Context**: After 10 minutes, 5 people left the room.\n",
      "**Correct Answer**: 5 people\n",
      "**Incorrect Response**: The context does not provide a clear answer, yet it states that 5 people left the room.\n",
      "**Errors in response**:\n",
      "Error: The response contradicts itself and initially provide wrong information\n",
      "Error Location: The context does not provide a clear answer, yet it states that\n",
      "Error Correction: Delete 'The context does not provide a clear answer, yet it states that'\n",
      "**Corrected Response**: The context does not provide a clear answer, yet it states that<DEL_S> 5 people left the room.\n",
      "\n",
      "## Example 6: Complete Rewrite\n",
      "**Question**: What is the primary cause of Earth's seasons?\n",
      "**Context**: The Earth's tilt on its axis is the primary cause of the seasons.\n",
      "**Correct Answer**: The tilt of the Earth's axis causes the seasons.\n",
      "**Incorrect Response**: The distance from the sun causes the seasons. It gets warmer when we are closer to the sun.\n",
      "**Errors in response**:\n",
      "Error: Fundamentally incorrect reasoning\n",
      "Error Location: The distance from the sun causes the seasons. It gets warmer when we are closer to the sun.\n",
      "Error Correction: Delete the entire response and replace it with the correct answer.\n",
      "**Corrected Response**: The distance from the sun causes the seasons. It gets warmer when we are closer to the sun.<DEL_A> The tilt of the Earth's axis causes the seasons.\n",
      "\n",
      "# Your Turn:\n",
      "**Question**: Ann Arbor ranks 1st among what goods sold?\n",
      "**Context**: '''The Ann Arbor Hands-On Museum is located in a renovated and expanded historic downtown fire station. Multiple art galleries exist in the city, notably in the downtown area and around the University of Michigan campus. Aside from a large restaurant scene in the Main Street, South State Street, and South University Avenue areas, Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The Ann Arbor District Library maintains four branch outlets in addition to its main downtown building. The city is also home to the Gerald R. Ford Presidential Library.'''\n",
      "**Correct Answer**: ['books']\n",
      "**Incorrect Response**: According to the context, Ann Arbor ranks 1st among booksellers.\n",
      "**Errors in response**:\n",
      "Error: The response states that Ann Arbor ranks 1st among booksellers, but the context states that Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The correct answer is books.\n",
      "Error Location: booksellers\n",
      "How to fix the error: Delete \"booksellers\" and add \"books\".\n",
      "\n",
      "**Corrected Response**:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-22 20:30:53.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.context_qa_correction_agent\u001b[0m:\u001b[36mcorrect_response\u001b[0m:\u001b[36m155\u001b[0m - \u001b[1mCorrected responses: ['Based on the context,<DEL_W> Ann Arbor ranks 1st among books<DEL_W> sold.', 'According to the context, Ann Arbor ranks 1st among booksellers<DEL_W> books.']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a meticulous AI Verification Engine. Your sole purpose is to determine if a `Generated Answer` is correct by strictly comparing it against a `Correct Answer` and its supporting `Context`.\n",
      "\n",
      "# Task Inputs\n",
      "**Question**: Ann Arbor ranks 1st among what goods sold?\n",
      "**Context**: '''The Ann Arbor Hands-On Museum is located in a renovated and expanded historic downtown fire station. Multiple art galleries exist in the city, notably in the downtown area and around the University of Michigan campus. Aside from a large restaurant scene in the Main Street, South State Street, and South University Avenue areas, Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The Ann Arbor District Library maintains four branch outlets in addition to its main downtown building. The city is also home to the Gerald R. Ford Presidential Library.'''\n",
      "**Correct Answer**: ['books']\n",
      "**Generated Answer**: Based on the Ann Arbor ranks 1st among sold.\n",
      "\n",
      "# Evaluation Criteria\n",
      "A `Generated Answer` is considered **CORRECT** if and only if it meets ALL four of the following conditions:\n",
      "1. **Semantic Equivalence**: Its core meaning is identical to the `Correct Answer`.\n",
      "2. **Contextual Grounding**: All information within it is fully supported by the provided `Context`.\n",
      "3. **Factual Accuracy**: It contains no factual errors or contradictions.\n",
      "4. **Grammatical Integrity**: It is free of grammatical errors.\n",
      "\n",
      "If the `Generated Answer` fails even one of these criteria, it is considered **INCORRECT**.\n",
      "\n",
      "# Output Requirement\n",
      "Your entire response MUST be a single word:\n",
      "- `True` if the `Generated Answer` is CORRECT (meeting all four criteria).\n",
      "- `False` if the `Generated Answer` is INCORRECT (failing one or more criteria).\n",
      "\n",
      "Do not include any other text, explanations, markdown, or formatting.\n",
      "\n",
      "Response (True/False): \n",
      "You are a meticulous AI Verification Engine. Your sole purpose is to determine if a `Generated Answer` is correct by strictly comparing it against a `Correct Answer` and its supporting `Context`.\n",
      "\n",
      "# Task Inputs\n",
      "**Question**: Ann Arbor ranks 1st among what goods sold?\n",
      "**Context**: '''The Ann Arbor Hands-On Museum is located in a renovated and expanded historic downtown fire station. Multiple art galleries exist in the city, notably in the downtown area and around the University of Michigan campus. Aside from a large restaurant scene in the Main Street, South State Street, and South University Avenue areas, Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The Ann Arbor District Library maintains four branch outlets in addition to its main downtown building. The city is also home to the Gerald R. Ford Presidential Library.'''\n",
      "**Correct Answer**: ['books']\n",
      "**Generated Answer**: According to the context, Ann Arbor ranks 1st among books.\n",
      "\n",
      "# Evaluation Criteria\n",
      "A `Generated Answer` is considered **CORRECT** if and only if it meets ALL four of the following conditions:\n",
      "1. **Semantic Equivalence**: Its core meaning is identical to the `Correct Answer`.\n",
      "2. **Contextual Grounding**: All information within it is fully supported by the provided `Context`.\n",
      "3. **Factual Accuracy**: It contains no factual errors or contradictions.\n",
      "4. **Grammatical Integrity**: It is free of grammatical errors.\n",
      "\n",
      "If the `Generated Answer` fails even one of these criteria, it is considered **INCORRECT**.\n",
      "\n",
      "# Output Requirement\n",
      "Your entire response MUST be a single word:\n",
      "- `True` if the `Generated Answer` is CORRECT (meeting all four criteria).\n",
      "- `False` if the `Generated Answer` is INCORRECT (failing one or more criteria).\n",
      "\n",
      "Do not include any other text, explanations, markdown, or formatting.\n",
      "\n",
      "Response (True/False): \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-22 20:30:54.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.context_qa_correction_agent\u001b[0m:\u001b[36mverify_corrected_response\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mVerified responses: ['False', 'True']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent = ContextQACorrectionAgent(estimate_only=False)\n",
    "\n",
    "state = agent.model.invoke({\n",
    "    \"question\": question,\n",
    "    \"context\": context,\n",
    "    \"answer\": answer,\n",
    "    \"responses\": responses,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b70d72f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Ann Arbor ranks 1st among what goods sold?',\n",
       " 'context': 'The Ann Arbor Hands-On Museum is located in a renovated and expanded historic downtown fire station. Multiple art galleries exist in the city, notably in the downtown area and around the University of Michigan campus. Aside from a large restaurant scene in the Main Street, South State Street, and South University Avenue areas, Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The Ann Arbor District Library maintains four branch outlets in addition to its main downtown building. The city is also home to the Gerald R. Ford Presidential Library.',\n",
       " 'answer': ['books'],\n",
       " 'responses': ('Based on the context, Ann Arbor ranks 1st among books sold.',\n",
       "  'According to the context, Ann Arbor ranks 1st among booksellers.'),\n",
       " 'errors': [[Error(error_description='The response states that Ann Arbor ranks 1st among booksellers, but the context states that Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The correct answer is books.', error_location='booksellers', error_correction='Delete \"booksellers\" and add \"books\".')]],\n",
       " 'corrected_responses': ['According to the context, Ann Arbor ranks 1st among booksellers<DEL_W> books.'],\n",
       " 'verified_response_mask': [False, True]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
